from keras.preprocessing import sequence
from keras.preprocessing import text
import amazon
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM
from keras.layers import Conv1D, Flatten
from keras.preprocessing import text
import wandb
from wandb.keras import WandbCallback

wandb.init()
config = wandb.config
config.vocab_size = 1000

(train_summary, train_review_text, train_labels), (test_summary, test_review_text, test_labels) = amazon.load_amazon()

config.vocab_size = 1000
config.maxlen = 1000
config.batch_size = 32
config.embedding_dims = 50
config.filters = 250
config.kernel_size = 3
config.hidden_dims = 250
config.epochs = 10

(X_train, y_train), (X_test, y_test) = (train_summary, train_labels), (test_summary, test_labels)
print("Review", X_train[0])
print("Label", y_train[0])

tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)
print(X_train.shape)
print("After pre-processing", X_train[0])

model = Sequential()
model.add(Embedding(config.vocab_size,
                    config.embedding_dims,
                    input_length=config.maxlen))
model.add(Dropout(0.5))
model.add(Conv1D(config.filters,
                 config.kernel_size,
                 padding='valid',
                 activation='relu'))
model.add(Flatten())
model.add(Dense(config.hidden_dims, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.fit(X_train, train_labels, batch_size=100, epochs=10, validation_data=(X_test, test_labels),
    callbacks=[WandbCallback()])
import os
import json

def load_amazon():
    filename = 'reviews_Video_Games_5.json'
    train_summary = []
    train_review_text = []
    train_labels = []
    
    test_summary = []
    test_review_text = []
    test_labels = []
    
    with open(filename, 'r') as f:
        for (i, line) in enumerate(f):
            data = json.loads(line)
            
            if data['overall'] == 3:
                next
            elif data['overall'] == 4 or data['overall'] == 5:
                label = 1
            elif data['overall'] == 1 or data['overall'] == 2:
                label = 0
            else:
                raise Exception("Unexpected value " + str(data['overall']))
                
            summary = data['summary']
            review_text = data['reviewText']
            
            if (i % 10 == 0):
                test_summary.append(summary)
                test_review_text.append(review_text)
                test_labels.append(label)
            else:
                train_summary.append(summary)
                train_review_text.append(review_text)
                train_labels.append(label)
                
    return (train_summary, train_review_text, train_labels), (test_summary, test_review_text, test_labels)

load_amazon()import scipy.io.wavfile
from os.path import expanduser
import os
import array
from pylab import *
import scipy.signal
import scipy
import wave
import numpy as np
import time
import sys
import math
import matplotlib
import subprocess

# Author: Brian K. Vogel
# brian.vogel@gmail.com

fft_size = 2048
iterations = 300
hopsamp = fft_size // 8


def ensure_audio():
    if not os.path.exists("audio"):
        print("Downloading audio dataset...")
        subprocess.check_output(
            "curl -SL https://storage.googleapis.com/wandb/audio.tar.gz | tar xz", shell=True)


def griffin_lim(stft, scale):
    # Undo the rescaling.
    stft_modified_scaled = stft / scale
    stft_modified_scaled = stft_modified_scaled**0.5
    # Use the Griffin&Lim algorithm to reconstruct an audio signal from the
    # magnitude spectrogram.
    x_reconstruct = reconstruct_signal_griffin_lim(stft_modified_scaled,
                                                   fft_size, hopsamp,
                                                   iterations)
    # The output signal must be in the range [-1, 1], otherwise we need to clip or normalize.
    max_sample = np.max(abs(x_reconstruct))
    if max_sample > 1.0:
        x_reconstruct = x_reconstruct / max_sample
    return x_reconstruct


def hz_to_mel(f_hz):
    """Convert Hz to mel scale.

    This uses the formula from O'Shaugnessy's book.
    Args:
        f_hz (float): The value in Hz.

    Returns:
        The value in mels.
    """
    return 2595*np.log10(1.0 + f_hz/700.0)


def mel_to_hz(m_mel):
    """Convert mel scale to Hz.

    This uses the formula from O'Shaugnessy's book.
    Args:
        m_mel (float): The value in mels

    Returns:
        The value in Hz
    """
    return 700*(10**(m_mel/2595) - 1.0)


def fft_bin_to_hz(n_bin, sample_rate_hz, fft_size):
    """Convert FFT bin index to frequency in Hz.

    Args:
        n_bin (int or float): The FFT bin index.
        sample_rate_hz (int or float): The sample rate in Hz.
        fft_size (int or float): The FFT size.

    Returns:
        The value in Hz.
    """
    n_bin = float(n_bin)
    sample_rate_hz = float(sample_rate_hz)
    fft_size = float(fft_size)
    return n_bin*sample_rate_hz/(2.0*fft_size)


def hz_to_fft_bin(f_hz, sample_rate_hz, fft_size):
    """Convert frequency in Hz to FFT bin index.

    Args:
        f_hz (int or float): The frequency in Hz.
        sample_rate_hz (int or float): The sample rate in Hz.
        fft_size (int or float): The FFT size.

    Returns:
        The FFT bin index as an int.
    """
    f_hz = float(f_hz)
    sample_rate_hz = float(sample_rate_hz)
    fft_size = float(fft_size)
    fft_bin = int(np.round((f_hz*2.0*fft_size/sample_rate_hz)))
    if fft_bin >= fft_size:
        fft_bin = fft_size-1
    return fft_bin


def make_mel_filterbank(min_freq_hz, max_freq_hz, mel_bin_count,
                        linear_bin_count, sample_rate_hz):
    """Create a mel filterbank matrix.

    Create and return a mel filterbank matrix `filterbank` of shape (`mel_bin_count`,
    `linear_bin_couont`). The `filterbank` matrix can be used to transform a
    (linear scale) spectrum or spectrogram into a mel scale spectrum or
    spectrogram as follows:

    `mel_scale_spectrum` = `filterbank`*'linear_scale_spectrum'

    where linear_scale_spectrum' is a shape (`linear_bin_count`, `m`) and
    `mel_scale_spectrum` is shape ('mel_bin_count', `m`) where `m` is the number
    of spectral time slices.

    Likewise, the reverse-direction transform can be performed as:

    'linear_scale_spectrum' = filterbank.T`*`mel_scale_spectrum`

    Note that the process of converting to mel scale and then back to linear
    scale is lossy.

    This function computes the mel-spaced filters such that each filter is triangular
    (in linear frequency) with response 1 at the center frequency and decreases linearly
    to 0 upon reaching an adjacent filter's center frequency. Note that any two adjacent
    filters will overlap having a response of 0.5 at the mean frequency of their
    respective center frequencies.

    Args:
        min_freq_hz (float): The frequency in Hz corresponding to the lowest
            mel scale bin.
        max_freq_hz (flloat): The frequency in Hz corresponding to the highest
            mel scale bin.
        mel_bin_count (int): The number of mel scale bins.
        linear_bin_count (int): The number of linear scale (fft) bins.
        sample_rate_hz (float): The sample rate in Hz.

    Returns:
        The mel filterbank matrix as an 2-dim Numpy array.
    """
    min_mels = hz_to_mel(min_freq_hz)
    max_mels = hz_to_mel(max_freq_hz)
    # Create mel_bin_count linearly spaced values between these extreme mel values.
    mel_lin_spaced = np.linspace(min_mels, max_mels, num=mel_bin_count)
    # Map each of these mel values back into linear frequency (Hz).
    center_frequencies_hz = np.array([mel_to_hz(n) for n in mel_lin_spaced])
    mels_per_bin = float(max_mels - min_mels)/float(mel_bin_count - 1)
    mels_start = min_mels - mels_per_bin
    hz_start = mel_to_hz(mels_start)
    fft_bin_start = hz_to_fft_bin(hz_start, sample_rate_hz, linear_bin_count)
    #print('fft_bin_start: ', fft_bin_start)
    mels_end = max_mels + mels_per_bin
    hz_stop = mel_to_hz(mels_end)
    fft_bin_stop = hz_to_fft_bin(hz_stop, sample_rate_hz, linear_bin_count)
    #print('fft_bin_stop: ', fft_bin_stop)
    # Map each center frequency to the closest fft bin index.
    linear_bin_indices = np.array([hz_to_fft_bin(
        f_hz, sample_rate_hz, linear_bin_count) for f_hz in center_frequencies_hz])
    # Create filterbank matrix.
    filterbank = np.zeros((mel_bin_count, linear_bin_count))
    for mel_bin in range(mel_bin_count):
        center_freq_linear_bin = int(linear_bin_indices[mel_bin].item())
        # Create a triangular filter having the current center freq.
        # The filter will start with 0 response at left_bin (if it exists)
        # and ramp up to 1.0 at center_freq_linear_bin, and then ramp
        # back down to 0 response at right_bin (if it exists).

        # Create the left side of the triangular filter that ramps up
        # from 0 to a response of 1 at the center frequency.
        if center_freq_linear_bin > 1:
            # It is possible to create the left triangular filter.
            if mel_bin == 0:
                # Since this is the first center frequency, the left side
                # must start ramping up from linear bin 0 or 1 mel bin before the center freq.
                left_bin = max(0, fft_bin_start)
            else:
                # Start ramping up from the previous center frequency bin.
                left_bin = int(linear_bin_indices[mel_bin - 1].item())
            for f_bin in range(left_bin, center_freq_linear_bin+1):
                if (center_freq_linear_bin - left_bin) > 0:
                    response = float(f_bin - left_bin) / \
                        float(center_freq_linear_bin - left_bin)
                    filterbank[mel_bin, f_bin] = response
        # Create the right side of the triangular filter that ramps down
        # from 1 to 0.
        if center_freq_linear_bin < linear_bin_count-2:
            # It is possible to create the right triangular filter.
            if mel_bin == mel_bin_count - 1:
                # Since this is the last mel bin, we must ramp down to response of 0
                # at the last linear freq bin.
                right_bin = min(linear_bin_count - 1, fft_bin_stop)
            else:
                right_bin = int(linear_bin_indices[mel_bin + 1].item())
            for f_bin in range(center_freq_linear_bin, right_bin+1):
                if (right_bin - center_freq_linear_bin) > 0:
                    response = float(right_bin - f_bin) / \
                        float(right_bin - center_freq_linear_bin)
                    filterbank[mel_bin, f_bin] = response
        filterbank[mel_bin, center_freq_linear_bin] = 1.0

    return filterbank


def stft_for_reconstruction(x, fft_size, hopsamp):
    """Compute and return the STFT of the supplied time domain signal x.

    Args:
        x (1-dim Numpy array): A time domain signal.
        fft_size (int): FFT size. Should be a power of 2, otherwise DFT will be used.
        hopsamp (int):

    Returns:
        The STFT. The rows are the time slices and columns are the frequency bins.
    """
    window = np.hanning(fft_size)
    fft_size = int(fft_size)
    hopsamp = int(hopsamp)
    return np.array([np.fft.rfft(window*x[i:i+fft_size])
                     for i in range(0, len(x)-fft_size, hopsamp)])


def istft_for_reconstruction(X, fft_size, hopsamp):
    """Invert a STFT into a time domain signal.

    Args:
        X (2-dim Numpy array): Input spectrogram. The rows are the time slices and columns are the frequency bins.
        fft_size (int):
        hopsamp (int): The hop size, in samples.

    Returns:
        The inverse STFT.
    """
    fft_size = int(fft_size)
    hopsamp = int(hopsamp)
    window = np.hanning(fft_size)
    time_slices = X.shape[0]
    len_samples = int(time_slices*hopsamp + fft_size)
    x = np.zeros(len_samples)
    for n, i in enumerate(range(0, len(x)-fft_size, hopsamp)):
        x[i:i+fft_size] += window*np.real(np.fft.irfft(X[n]))
    return x


def get_signal(in_file, expected_fs=44100):
    """Load a wav file.

    If the file contains more than one channel, return a mono file by taking
    the mean of all channels.

    If the sample rate differs from the expected sample rate (default is 44100 Hz),
    raise an exception.

    Args:
        in_file: The input wav file, which should have a sample rate of `expected_fs`.
        expected_fs (int): The expected sample rate of the input wav file.

    Returns:
        The audio siganl as a 1-dim Numpy array. The values will be in the range [-1.0, 1.0]. fixme ( not yet)
    """
    fs, y = scipy.io.wavfile.read(in_file)
    num_type = y[0].dtype
    if num_type == 'int16':
        y = y*(1.0/32768)
    elif num_type == 'int32':
        y = y*(1.0/2147483648)
    elif num_type == 'float32':
        # Nothing to do
        pass
    elif num_type == 'uint8':
        raise Exception('8-bit PCM is not supported.')
    else:
        raise Exception('Unknown format.')
    if fs != expected_fs:
        raise Exception('Invalid sample rate.')
    if y.ndim == 1:
        return y
    else:
        return y.mean(axis=1)


def reconstruct_signal_griffin_lim(magnitude_spectrogram, fft_size, hopsamp, iterations):
    """Reconstruct an audio signal from a magnitude spectrogram.

    Given a magnitude spectrogram as input, reconstruct
    the audio signal and return it using the Griffin-Lim algorithm from the paper:
    "Signal estimation from modified short-time fourier transform" by Griffin and Lim,
    in IEEE transactions on Acoustics, Speech, and Signal Processing. Vol ASSP-32, No. 2, April 1984.

    Args:
        magnitude_spectrogram (2-dim Numpy array): The magnitude spectrogram. The rows correspond to the time slices
            and the columns correspond to frequency bins.
        fft_size (int): The FFT size, which should be a power of 2.
        hopsamp (int): The hope size in samples.
        iterations (int): Number of iterations for the Griffin-Lim algorithm. Typically a few hundred
            is sufficient.

    Returns:
        The reconstructed time domain signal as a 1-dim Numpy array.
    """
    time_slices = magnitude_spectrogram.shape[0]
    len_samples = int(time_slices*hopsamp + fft_size)
    # Initialize the reconstructed signal to noise.
    x_reconstruct = np.random.randn(len_samples)
    n = iterations  # number of iterations of Griffin-Lim algorithm.
    while n > 0:
        n -= 1
        reconstruction_spectrogram = stft_for_reconstruction(
            x_reconstruct, fft_size, hopsamp)
        reconstruction_angle = np.angle(reconstruction_spectrogram)
        # Discard magnitude part of the reconstruction and use the supplied magnitude spectrogram instead.
        proposal_spectrogram = magnitude_spectrogram * \
            np.exp(1.0j*reconstruction_angle)
        prev_x = x_reconstruct
        x_reconstruct = istft_for_reconstruction(
            proposal_spectrogram, fft_size, hopsamp)
        diff = sqrt(sum((x_reconstruct - prev_x)**2)/x_reconstruct.size)
        #print('Reconstruction iteration: {}/{} RMSE: {} '.format(iterations - n, iterations, diff))
    return x_reconstruct


def save_audio_to_file(x, sample_rate, outfile='out.wav'):
    """Save a mono signal to a file.

    Args:
        x (1-dim Numpy array): The audio signal to save. The signal values should be in the range [-1.0, 1.0].
        sample_rate (int): The sample rate of the signal, in Hz.
        outfile: Name of the file to save.

    """
    x_max = np.max(abs(x))
    assert x_max <= 1.0, 'Input audio value is out of range. Should be in the range [-1.0, 1.0].'
    x = x*32767.0
    data = array.array('h')
    for i in range(len(x)):
        cur_samp = int(round(x[i]))
        data.append(cur_samp)
    f = wave.open(outfile, 'w')
    f.setparams((1, 2, sample_rate, 0, "NONE", "Uncompressed"))
    f.writeframes(data.tostring())
    f.close()
from keras import backend as K
from keras.callbacks import Callback, TensorBoard
from keras.models import Model, Sequential, load_model
from keras import layers
import numpy as np
import glob
import audio_utilities
import wandb
from wandb.keras import WandbCallback
from keras import optimizers
import os
wandb.init()

sample_rate = 8000
audio_utilities.ensure_audio()
if not os.path.exists("cache"):
    raise ValueError("You must run python preprocess before training")


def load_data():
    print("Loading data...")
    data = {}
    X_train, y_train = [], []
    clean = np.load('cache/clean.npy')
    data['clean'] = clean[:, 1]
    data['dirty'] = []
    for f in glob.glob("cache/*"):
        spects = np.load(f)
        key = f.split("/")[-1]
        data['dirty'].extend(spects[:, 1])
        if key == "clean":
            continue
        else:
            X_train.extend(spects[:, 0])
            y_train.extend(clean[:, 0])
    return np.expand_dims(np.array(X_train), -1), np.expand_dims(np.array(y_train), -1), data


class Audio(Callback):
    def on_epoch_end(self, *args):
        validation_X = self.validation_data[0]
        validation_y = self.validation_data[1]
        val_scales = scales["dirty"][:20]
        validation_length = len(validation_X)
        indices = np.random.choice(
            validation_length, 1, replace=False)
        predictions = self.model.predict(validation_X[indices])
        print("Min: ", predictions.min(), "Max: ", predictions.max())
        predictions = predictions.clip(0, 1)  # np.max(abs(predictions))
        norm_pred = []
        norm_in = []
        clean_in = []
        for i, idx in enumerate(indices):
            scale = val_scales[idx]
            pred = np.squeeze(predictions[i])
            norm = np.squeeze(validation_X[idx])
            clean = np.squeeze(validation_y[idx])
            norm_pred.append(audio_utilities.griffin_lim(pred, scale))
            norm_in.append(audio_utilities.griffin_lim(norm, scale))
            clean_in.append(audio_utilities.griffin_lim(clean, scale))

        wandb.log({
            "clean_audio": [wandb.Audio(audio, sample_rate=sample_rate) for audio in clean_in],
            "noisy_audio": [wandb.Audio(audio, sample_rate=sample_rate) for audio in norm_in],
            "audio": [wandb.Audio(audio, sample_rate=sample_rate)
                      for audio in norm_pred]}, commit=False)


X_train, y_train, scales = load_data()

model = Sequential()
model.add(layers.Conv2D(64, 3, padding="same",
                        activation="relu", use_bias=False, input_shape=(X_train[0].shape)))
model.add(layers.Conv2D(32, 3, strides=(2, 2),
                        padding="same", activation="relu", use_bias=False))
model.add(layers.Conv2D(2, 3, strides=(2, 2),
                        padding="same", activation="relu", use_bias=False))
model.add(layers.UpSampling2D(2))
model.add(layers.Conv2D(32, 3, padding="same",
                        activation="relu", use_bias=False))
model.add(layers.UpSampling2D(2))
model.add(layers.Conv2D(64, 3, padding="same",
                        activation="relu", use_bias=False))
model.add(layers.Conv2D(1, 3, padding="same", use_bias=False))
model.add(layers.Cropping2D(cropping=((0, 0), (0, 3))))
model.summary()

model.compile(optimizer="adagrad", loss="mse")

model.fit(X_train[20:], y_train[20:], epochs=60, batch_size=64,
          validation_data=(X_train[:20], y_train[:20]), callbacks=[Audio(), WandbCallback(data_type="image")])
import requests
import os

midis = [
    "https://bitmidi.com/uploads/73866.mid",
    "https://bitmidi.com/uploads/73865.mid",
    "https://bitmidi.com/uploads/73867.mid"
]

os.makedirs("midi_songs", exist_ok=True)
if os.path.exists("data/notes"):
    os.remove("data/notes")

i = 0
for url in midis:
    try:
        r = requests.get(url, allow_redirects=True)
        open("midi_songs/" + url.split("/")[-1], "wb").write(r.content)
        i += 1
    except Exception:
        print("Failed to download %s" % url)

print("Downloaded %i midi files, you can now run `python gru-composer.py`" % i)
""" This module prepares midi file data and feeds it to the neural
    network for training """
import glob
import pickle
import numpy as np
import tensorflow as tf
from music21 import converter, instrument, note, chord, stream
import os
import subprocess
import wandb
import base64
wandb.init()


def ensure_midi(dataset="mario"):
    if dataset == "custom":
        if not os.path.exists("midi_songs"):
            raise ValueError(
                "Couldn't find custom soundtrack, please run python create_soundtrack.py")
        else:
            return True
    if not os.path.exists("data/%s" % dataset):
        print("Downloading %s dataset..." % dataset)
        subprocess.check_output(
            "curl -SL https://storage.googleapis.com/wandb/%s.tar.gz | tar xz" % dataset, shell=True)  # finalfantasy, hiphop, mario
        open("data/%s" % dataset, "w").close()


def train_network():
    """ Train a Neural Network to generate music """
    notes = get_notes()

    # get amount of pitch names
    n_vocab = len(set(notes))

    network_input, network_output = prepare_sequences(notes, n_vocab)

    model = create_network(network_input, n_vocab)

    train(model, network_input, network_output)


def get_notes():
    """ Get all the notes and chords from the midi files in the ./midi_songs directory """
    notes = []
    if os.path.exists("data/notes"):
        return pickle.load(open("data/notes", "rb"))

    for file in glob.glob("midi_songs/*.mid"):
        try:
            midi = converter.parse(file)
        except TypeError:
            print("Invalid file %s" % file)
            continue

        print("Parsing %s" % file)

        notes_to_parse = None

        try:  # file has instrument parts
            s2 = instrument.partitionByInstrument(midi)
            notes_to_parse = s2.parts[0].recurse()
        except:  # file has notes in a flat structure
            notes_to_parse = midi.flat.notes

        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes.append('.'.join(str(n) for n in element.normalOrder))

    with open('data/notes', 'wb') as filepath:
        pickle.dump(notes, filepath)

    return notes


def prepare_sequences(notes, n_vocab):
    """ Prepare the sequences used by the Neural Network """
    sequence_length = 100

    # get all pitch names
    pitchnames = sorted(set(item for item in notes))

    # create a dictionary to map pitches to integers
    note_to_int = dict((note, number)
                       for number, note in enumerate(pitchnames))

    network_input = []
    network_output = []

    # create input sequences and the corresponding outputs
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        network_output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)

    # reshape the input into a format compatible with LSTM layers
    network_input = np.reshape(
        network_input, (n_patterns, sequence_length, 1))
    # normalize input
    network_input = network_input / float(n_vocab)

    network_output = tf.keras.utils.to_categorical(network_output)

    return (network_input, network_output)


def create_network(network_input, n_vocab):
    """ create the structure of the neural network """
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.GRU(
        256,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        return_sequences=True
    ))
    model.add(tf.keras.layers.Dropout(0.3))
    model.add(tf.keras.layers.GRU(128, return_sequences=True))
    model.add(tf.keras.layers.Dropout(0.3))
    model.add(tf.keras.layers.GRU(64))
    model.add(tf.keras.layers.Dense(256))
    model.add(tf.keras.layers.Dropout(0.3))
    model.add(tf.keras.layers.Dense(n_vocab))
    model.add(tf.keras.layers.Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    return model


class Midi(tf.keras.callbacks.Callback):
    """
    Callback for sampling a midi file
    """

    def sample(self, preds, temperature=1):
        # helper function to sample an index from a probability array
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        probas = np.random.multinomial(1, preds, 1)
        return np.argmax(probas)

    def generate_notes(self, network_input, pitchnames, n_vocab):
        """ Generate notes from the neural network based on a sequence of notes """
        # pick a random sequence from the input as a starting point for the prediction
        model = self.model
        start = np.random.randint(0, len(network_input)-1)

        int_to_note = dict((number, note)
                           for number, note in enumerate(pitchnames))

        pattern = list(network_input[start])
        prediction_output = []

        # generate 500 notes
        for note_index in range(500):
            prediction_input = np.reshape(pattern, (1, len(pattern), 1))
            prediction_input = prediction_input / float(n_vocab)

            prediction = model.predict(prediction_input, verbose=0)

            index = self.sample(prediction[0], temperature=0.5)  # np.argmax
            result = int_to_note[index]
            prediction_output.append(result)

            pattern.append(index)
            pattern = pattern[1:len(pattern)]

        return prediction_output

    def create_midi(self, prediction_output):
        """ convert the output from the prediction to notes and create a midi file
            from the notes """
        offset = 0
        output_notes = []

        # create note and chord objects based on the values generated by the model
        for pattern in prediction_output:
            # pattern is a chord
            if ('.' in pattern) or pattern.isdigit():
                notes_in_chord = pattern.split('.')
                notes = []
                for current_note in notes_in_chord:
                    new_note = note.Note(int(current_note))
                    new_note.storedInstrument = instrument.Piano()
                    notes.append(new_note)
                new_chord = chord.Chord(notes)
                new_chord.offset = offset
                output_notes.append(new_chord)
            # pattern is a note
            else:
                new_note = note.Note(pattern)
                new_note.offset = offset
                new_note.storedInstrument = instrument.Piano()
                output_notes.append(new_note)

            # increase offset each iteration so that notes do not stack
            offset += 0.5

        midi_stream = stream.Stream(output_notes)

        return midi_stream.write('midi')

    def on_epoch_end(self, *args):
        notes = get_notes()
        # Get all pitch names
        pitchnames = sorted(set(item for item in notes))
        # Get all pitch names
        n_vocab = len(set(notes))
        network_input, normalized_input = prepare_sequences(
            notes, n_vocab)
        music = self.generate_notes(network_input, pitchnames, n_vocab)
        midi = self.create_midi(music)
        midi = open(midi, "rb")
        data = "data:audio/midi;base64,%s" % base64.b64encode(
            midi.read()).decode("utf8")
        wandb.log({
            "midi": wandb.Html("""
                <script type="text/javascript" src="//www.midijs.net/lib/midi.js"></script>
                <button onClick="MIDIjs.play('%s')">Play midi</button>
                <button onClick="MIDIjs.stop()">Stop Playback</button>
            """ % data)
        }, commit=False)


def train(model, network_input, network_output):
    """ train the neural network """
    filepath = "mozart.hdf5"
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        filepath,
        monitor='loss',
        verbose=0,
        save_best_only=True,
        mode='min'
    )
    callbacks_list = [Midi(), wandb.keras.WandbCallback(), checkpoint]

    model.fit(network_input, network_output, epochs=200,
              batch_size=128, callbacks=callbacks_list)


if __name__ == '__main__':
    ensure_midi("mario")
    train_network()
import os
import numpy as np

import audio_utilities

data = {}
print("Loading audio...")
fft_size = 2048
sample_rate = 8000
max_len = 80
mel = False
for root, dirs, files in os.walk("./audio"):
    if len(files) == 0:
        continue
    group = os.path.split(root)[-1]
    if group == "audio":
        continue
    data[group] = []
    for f in sorted(files):
        if f.endswith(".wav"):
            input_signal = audio_utilities.get_signal(
                os.path.join(root, f), expected_fs=sample_rate)
            # Hopsamp is the number of samples that the analysis window is shifted after
            # computing the FFT. For example, if the sample rate is 44100 Hz and hopsamp is
            # 256, then there will be approximately 44100/256 = 172 FFTs computed per second
            # and thus 172 spectral slices (i.e., columns) per second in the spectrogram.
            hopsamp = fft_size // 8

            # Compute the Short-Time Fourier Transform (STFT) from the audio file. This is a 2-dim Numpy array with
            # time_slices rows and frequency_bins columns. Thus, you will need to take the
            # transpose of this matrix to get the usual STFT which has frequency bins as rows
            # and time slices as columns.
            stft_full = audio_utilities.stft_for_reconstruction(input_signal,
                                                                fft_size, hopsamp)

            # If maximum length exceeds mfcc lengths then pad the remaining ones
            if (max_len > stft_full.shape[0]):
                pad_width = max_len - stft_full.shape[0]
                stft_pad = np.pad(stft_full, pad_width=(
                    (0, pad_width), (0, 0)), mode='constant')
            # Else cutoff the remaining parts
            else:
                stft_pad = stft_full[:max_len, :]
            # Note that the STFT is complex-valued. Therefore, to get the (magnitude)
            # spectrogram, we need to take the absolute value.
            stft_mag = abs(stft_pad)**2.0
            # Note that `stft_mag` only contains the magnitudes and so we have lost the
            # phase information.
            scale = 1.0 / np.amax(stft_mag)
            # print('Maximum value in the magnitude spectrogram: ', 1/scale)
            # Rescale to put all values in the range [0, 1].
            stft_mag *= scale

            if mel:
                min_freq_hz = 70
                max_freq_hz = 8000
                mel_bin_count = 200

                linear_bin_count = 1 + fft_size//2
                filterbank = audio_utilities.make_mel_filterbank(min_freq_hz, max_freq_hz, mel_bin_count,
                                                                 linear_bin_count, sample_rate)
                mel_spectrogram = np.dot(filterbank, stft_mag.T)
                inverted_mel_to_linear_freq_spectrogram = np.dot(
                    filterbank.T, mel_spectrogram)
                strf_mag = inverted_mel_to_linear_freq_spectrogram.T
                print(strf_mag.shape)
            data[group].append((stft_mag, scale))

    if not os.path.exists("cache"):
        os.mkdir("cache")
    np.save(os.path.join("cache", group + '.npy'), data[group])
from util import Images
import tensorflow as tf
import wandb

run = wandb.init()
config = run.config

config.encoding_dim = 10
config.epochs = 10

(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()

X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

encoder = tf.keras.models.Sequential()
encoder.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
encoder.add(tf.keras.layers.Dense(128, activation="relu"))
encoder.add(tf.keras.layers.Dense(64, activation="relu"))
encoder.add(tf.keras.layers.Dense(config.encoding_dim, activation="relu"))

decoder = tf.keras.models.Sequential()
decoder.add(tf.keras.layers.Dense(64, activation="relu",
                                  input_shape=(config.encoding_dim,)))
decoder.add(tf.keras.layers.Dense(128, activation="relu"))
decoder.add(tf.keras.layers.Dense(28*28, activation="sigmoid"))
decoder.add(tf.keras.layers.Reshape((28, 28)))

model = tf.keras.models.Sequential()
model.add(encoder)
model.add(decoder)

model.compile(optimizer='adam', loss='mse')

model.fit(X_train, X_train,
          epochs=config.epochs,
          validation_data=(X_test, X_test),
          callbacks=[Images(X_test), wandb.keras.WandbCallback(save_model="false")])

encoder.save('auto-encoder.h5')
decoder.save('auto-decoder.h5')
from util import Images
import wandb
import tensorflow as tf
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config

config.epochs = 30

(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()

X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)))
model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(2, 2))
model.add(tf.keras.layers.Conv2D(4, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(2, 2))
model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.UpSampling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(
    12, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.UpSampling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.Reshape((28, 28)))

model.compile(optimizer='adam', loss='mse')

model.fit(X_train, X_train,
          epochs=config.epochs,
          validation_data=(X_test, X_test),
          callbacks=[Images(X_test), wandb.keras.WandbCallback(save_model=False)])


model.save('auto-cnn.h5')
import matplotlib
matplotlib.use("Agg")  # noqa
import sys
import cv2
import numpy as np
import pandas as pd
import subprocess
import os
import pdb
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import manifold
import plotly.plotly as py
import plotly.graph_objs as go
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import Callback
from tensorflow.keras import backend as K
from tensorflow.keras.utils.generic_utils import get_custom_objects
import wandb
from wandb.keras import WandbCallback


wandb.init()
wandb.config.latent_dim = 2
wandb.config.labels = [str(i) for i in range(10)]  # ["Happy", "Sad"]
wandb.config.batch_size = 128
wandb.config.epochs = 25
wandb.config.conditional = True
wandb.config.latent_vis = False
wandb.config.dataset = "mnist"

EMOTIONS = ["Angry", "Disgust", "Fear", "Happy", "Sad", "Surprise", "Neutral"]


def load_fer2013(filter_emotions=[]):
    if not os.path.exists("fer2013"):
        print("Downloading the face emotion dataset...")
        subprocess.check_output(
            "curl -SL https://www.dropbox.com/s/opuvvdv3uligypx/fer2013.tar | tar xz", shell=True)
    print("Loading dataset into memory...")
    data = pd.read_csv("fer2013/fer2013.csv")
    if len(filter_emotions) > 0:
        data = data.loc[data["emotion"].isin(
            [EMOTIONS.index(f) for f in filter_emotions])]
    pixels = data['pixels'].tolist()
    width, height = 48, 48
    faces = []
    for pixel_sequence in pixels:
        face = np.asarray(pixel_sequence.split(
            ' '), dtype=np.uint8).reshape(width, height)
        face = cv2.resize(face.astype('uint8'), (width, height))
        faces.append(face.astype('float32'))

    faces = np.asarray(faces) / 255.
    faces = np.expand_dims(faces, -1)
    emotions = pd.get_dummies(data['emotion']).as_matrix()

    val_faces = faces[int(len(faces) * 0.8):]
    val_emotions = emotions[int(len(faces) * 0.8):]
    train_faces = faces[:int(len(faces) * 0.8)]
    train_emotions = emotions[:int(len(faces) * 0.8)]

    return train_faces, train_emotions, val_faces, val_emotions


def concat_label(args):
    '''
    Converts 2d labels to 3d, i.e. [[0,1]] = [[[0,0],[0,0]],[[1,1],[1,1]]]
    '''
    x, labels = args
    x_shape = K.int_shape(x)
    label_shape = K.int_shape(labels)
    output = K.reshape(labels, (K.shape(x)[0],
                                1, 1, label_shape[1]))
    output = K.repeat_elements(output, x_shape[1], axis=1)
    output = K.repeat_elements(output, x_shape[2], axis=2)

    return K.concatenate([x, output], axis=-1)


class ShowImages(Callback):
    '''
    Keras callback for logging predictions and a scatter plot of the latent dimension
    '''

    def on_epoch_end(self, epoch, logs):
        indicies = np.random.randint(X_test.shape[0], size=36)
        latent_idx = np.random.randint(X_test.shape[0], size=500)
        inputs = X_test[indicies]
        t_inputs = X_train[indicies]
        r_labels = y_test[indicies]
        rand_labels = np.random.randint(len(wandb.config.labels), size=35)
        # always add max label
        rand_labels = np.append(rand_labels, [len(wandb.config.labels) - 1])
        labels = keras.utils.to_categorical(rand_labels)
        t_labels = y_train[indicies]

        results = cae.predict([inputs, r_labels, labels])
        t_results = cae.predict([t_inputs, t_labels, t_labels])
        print("Max pixel value", t_results.max())
        latent = encoder.predict([X_test[latent_idx], y_test[latent_idx]])
        # Plot latent space
        if wandb.config.latent_vis:
            if wandb.config.latent_dim > 2:
                # latent_vis = manifold.TSNE(n_components=2, init='pca', random_state=0)
                latent_vis = PCA(n_components=2)
                X = latent_vis.fit_transform(latent)
            else:
                X = latent
            trace = go.Scatter(x=list(X[:, 0]), y=list(X[:, 1]),
                               mode='markers', showlegend=False,
                               marker=dict(color=list(np.argmax(y_test[latent_idx], axis=1)),
                                           colorscale='Viridis',
                                           size=8,
                                           showscale=True))
            fig = go.Figure(data=[trace])
            wandb.log({"latent_vis": fig}, commit=False)
        # Always log training images
        wandb.log({
            "train_images": [wandb.Image(
                np.hstack([t_inputs[i], res])) for i, res in enumerate(t_results)
            ]
        }, commit=False)

        # Log image conversion when conditional
        if wandb.config.conditional:
            wandb.log({
                "images": [wandb.Image(
                    np.hstack([inputs[i], res]), caption=" to ".join([
                        wandb.config.labels[np.argmax(r_labels[i])
                                            ], wandb.config.labels[np.argmax((labels)[i])]
                    ])) for i, res in enumerate(results)]}, commit=False)


def create_encoder(input_shape):
    '''
    Create an encoder with an optional class append to the channel.
    '''
    encoder_input = layers.Input(shape=input_shape)
    label_input = layers.Input(shape=(len(wandb.config.labels),))
    x = layers.Flatten()(encoder_input)
    if wandb.config.conditional:
        #x = layers.Lambda(concat_label, name="c")([encoder_input, label_input])
        x = layers.concatenate([x, label_input], axis=-1)

    x = layers.Dense(512, activation="relu")(x)
    output = layers.Dense(wandb.config.latent_dim, activation="relu")(x)

    return Model([encoder_input, label_input], output, name='encoder')


def create_categorical_decoder():
    '''
    Create the decoder with an optional class appended to the input.
    '''
    decoder_input = layers.Input(shape=(wandb.config.latent_dim,))
    label_input = layers.Input(shape=(len(wandb.config.labels),))
    if wandb.config.conditional:
        x = layers.concatenate([decoder_input, label_input], axis=-1)
    else:
        x = decoder_input
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dense(img_size * img_size, activation='sigmoid')(x)
    x = layers.Reshape((img_size, img_size, 1))(x)

    return Model([decoder_input, label_input], x, name='decoder')


if wandb.config.dataset == "emotions":
    # Load emotion dataset
    img_size = 48
    X_train, y_train, X_test, y_test = load_fer2013(
        filter_emotions=wandb.config.labels)
else:
    # Load mnist dataset
    img_size = 28
    from keras.datasets import mnist
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    X_train = X_train.astype('float32')
    X_train /= 255.
    X_test = X_test.astype('float32')
    X_test /= 255.

    # reshape input data
    X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)
    X_test = X_test.reshape(X_test.shape[0], img_size, img_size, 1)

    # Filter the dataset to the classes we want
    labels = [int(l) for l in wandb.config.labels]
    X_train = X_train[np.isin(y_train, labels)]
    X_test = X_test[np.isin(y_test, labels)]
    filtered_train = y_train[np.isin(y_train, labels)]
    filtered_test = y_test[np.isin(y_test, labels)]
    for i, label in enumerate(labels):
        filtered_train = np.where(filtered_train == label, i, filtered_train)
        filtered_test = np.where(filtered_test == label, i, filtered_test)
    y_train = keras.utils.to_categorical(filtered_train)
    y_test = keras.utils.to_categorical(filtered_test)

encoder = create_encoder(input_shape=(img_size, img_size, 1))
decoder = create_categorical_decoder()

image = layers.Input(shape=(img_size, img_size, 1))
true_label = layers.Input(shape=(len(wandb.config.labels),))
dest_label = layers.Input(shape=(len(wandb.config.labels),))
output = encoder([image, true_label])
t_decoded = decoder([output, dest_label])


cae = Model([image, true_label, dest_label], t_decoded, name='vae')
cae.compile(optimizer='rmsprop', loss="mse")

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

if __name__ == '__main__':
    cae.fit([X_train, y_train, y_train], X_train, epochs=wandb.config.epochs,
            shuffle=True, batch_size=wandb.config.batch_size, callbacks=[ShowImages(), WandbCallback()],
            validation_data=([X_test, y_test, y_test], X_test))
    encoder.save("encoder.h5")
    decoder.save("decoder.h5")
import numpy as np
import tensorflow as tf
import wandb
from util import Images


# Add random noise to images.
def add_noise(x_train, x_test):
    noise_factor = 0.5
    x_train_noisy = x_train + \
        np.random.normal(loc=0.0, scale=noise_factor, size=x_train.shape)
    x_test_noisy = x_test + \
        np.random.normal(loc=0.0, scale=noise_factor, size=x_test.shape)

    x_train_noisy = np.clip(x_train_noisy, 0., 1.)
    x_test_noisy = np.clip(x_test_noisy, 0., 1.)
    return x_train_noisy, x_test_noisy


# Set Hyper-parameters
run = wandb.init()
config = run.config
config.encoding_dim = 32
config.epochs = 10

# Load and normalize data
(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
(X_train_noisy, X_test_noisy) = add_noise(X_train, X_test)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(config.encoding_dim, activation='relu'))
model.add(tf.keras.layers.Dense(784, activation='sigmoid'))
model.add(tf.keras.layers.Reshape((28, 28)))
model.compile(optimizer='adam', loss='binary_crossentropy')


model.fit(X_train_noisy, X_train,
          epochs=config.epochs,
          validation_data=(X_test_noisy, X_test),
          callbacks=[Images(X_test_noisy), wandb.keras.WandbCallback(save_model=False)])


model.save("auto-denoise.h5")
import numpy as np
import wandb
import tensorflow as tf
from util import Images


# Add noise
def add_noise(x_train, x_test):
    noise_factor = 0.5
    x_train_noisy = x_train + noise_factor * \
        np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
    x_test_noisy = x_test + noise_factor * \
        np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

    x_train_noisy = np.clip(x_train_noisy, 0., 1.)
    x_test_noisy = np.clip(x_test_noisy, 0., 1.)
    return x_train_noisy, x_test_noisy


# Set Hyper-parameters
run = wandb.init()
config = run.config
config.encoding_dim = 32
config.epochs = 10

# Load and normalize data
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
(x_train_noisy, x_test_noisy) = add_noise(x_train, x_test)


model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)))
model.add(tf.keras.layers.Conv2D(
    32, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(
    16, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.UpSampling2D())
model.add(tf.keras.layers.Conv2D(
    1, (3, 3), padding='same', activation='sigmoid'))
model.add(tf.keras.layers.Reshape((28, 28)))
model.compile(optimizer='adam', loss='mse')

model.fit(x_train_noisy, x_train,
          epochs=config.epochs,
          validation_data=(x_test_noisy, x_test),
          callbacks=[Images(x_test_noisy), wandb.keras.WandbCallback(save_model=False)])

model.save("auto-denoise.h5")
import matplotlib
matplotlib.use('Agg')  # noqa
import matplotlib.pyplot as plt
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from matplotlib.figure import Figure
from tensorflow import keras
import numpy as np
import wandb


def fig2data(fig):
    """
    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it
    @param fig a matplotlib figure
    @return a numpy 3D array of RGBA values
    """
    # draw the renderer
    fig.canvas.draw()

    # Get the RGBA buffer from the figure
    w, h = fig.canvas.get_width_height()
    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)
    buf.shape = (w, h, 4)

    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode
    buf = np.roll(buf, 3, axis=2)
    return buf


def plot_results(models,
                 data,
                 batch_size=128,
                 model_name="vae_mnist"):
    """Plots labels and MNIST digits as function of 2-dim latent vector
    # Arguments:
        models (tuple): encoder and decoder models
        data (tuple): test data and label
        batch_size (int): prediction batch size
        model_name (string): which model is using this function
    """

    os.makedirs(model_name, exist_ok=True)

    filename = os.path.join(model_name, "vae_mean.png")


class PlotCallback(keras.callbacks.Callback):
    def __init__(self, encoder, decoder, data):
        self.encoder = encoder
        self.decoder = decoder
        self.x_test, self.y_test = data
        self.batch_size = 64

    def on_epoch_end(self, epoch, logs):

        # Generate a figure with matplotlib</font>
        figure = matplotlib.pyplot.figure(figsize=(10, 10))
        plt = figure.add_subplot(111)

        # display a 2D plot of the digit classes in the latent space
        z_mean, _, _ = self.encoder.predict(self.x_test,
                                            batch_size=self.batch_size)
        plt.scatter(z_mean[:, 0], z_mean[:, 1], c=self.y_test)
        # plt.colorbar()
        # plt.xlabel("z[0]")
        # plt.ylabel("z[1]")

        data = fig2data(figure)
        matplotlib.pyplot.close(figure)

        wandb.log({"scatter": wandb.Image(data)}, commit=False)

        # Generate a figure with matplotlib</font>
        figure = matplotlib.pyplot.figure(figsize=(10, 10))
        plt = figure.add_subplot(111)

        # display a 30x30 2D manifold of digits
        n = 30
        digit_size = 28
        fig = np.zeros((digit_size * n, digit_size * n))
        # linearly spaced coordinates corresponding to the 2D plot
        # of digit classes in the latent space
        grid_x = np.linspace(-4, 4, n)
        grid_y = np.linspace(-4, 4, n)[::-1]

        for i, yi in enumerate(grid_y):
            for j, xi in enumerate(grid_x):
                z_sample = np.array([[xi, yi]])
                x_decoded = self.decoder.predict(z_sample)
                digit = x_decoded[0].reshape(digit_size, digit_size)
                fig[i * digit_size: (i + 1) * digit_size,
                    j * digit_size: (j + 1) * digit_size] = digit

        start_range = digit_size // 2
        end_range = n * digit_size + start_range + 1
        pixel_range = np.arange(start_range, end_range, digit_size)
        sample_range_x = np.round(grid_x, 1)
        sample_range_y = np.round(grid_y, 1)
        #plt.xticks(pixel_range, sample_range_x)
        #plt.yticks(pixel_range, sample_range_y)
        # plt.xlabel("z[0]")
        # plt.ylabel("z[1]")
        plt.imshow(fig, cmap='Greys_r')

        data = fig2data(figure)
        matplotlib.pyplot.close(figure)

        wandb.log({"grid": wandb.Image(data)}, commit=False)
from keras.models import Model
from keras.models import load_model

from keras.datasets import mnist
import numpy as np
import cv2

(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

model = load_model('auto-denoise.h5')

def add_noise(x_train):
    noise_factor = 0.5
    x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
        
    x_train_noisy = np.clip(x_train_noisy, 0., 1.)
    return x_train_noisy

i = 0
while(True):

  k = cv2.waitKey(0)
  if k == 27:         # wait for ESC key to exit                                                                                                             cv2.destroyAllWindows()
    break

  input_img = x_test[i]

  if k == 32:   # space bar
      input_img = add_noise(input_img)

  output_img = model.predict(input_img.reshape(1,28,28))[0].reshape(28,28,1)
  cv2.imshow('input', input_img)
  cv2.imshow('output', output_img) 
  i+=1
 



from keras.models import Model
from keras.models import load_model

from keras.datasets import mnist
import numpy as np
import cv2

(x_train, _), (x_test, _) = mnist.load_data()

model = load_model('auto.h5')

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.


def add_noise(x_train):
    noise_factor = 0.5
    x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
        
    x_train_noisy = np.clip(x_train_noisy, 0., 1.)
    return x_train_noisy


i = 0
while(True):

  k = cv2.waitKey(0)
  if k == 27:         # wait for ESC key to exit                                                                                                             cv2.destroyAllWindows()
    break

  input_img = x_test[i]

  if k == 32:   # space bar
      input_img = add_noise(input_img)

  output_img = model.predict(input_img.reshape(1,28,28))[0].reshape(28,28,1)
  cv2.imshow('input', input_img)
  cv2.imshow('output', output_img) 
  i+=1
 



from tensorflow.keras.callbacks import Callback
import numpy as np
import wandb


class Images(Callback):
    def __init__(self, val_data, **kwargs):
        super(Images, self).__init__(**kwargs)
        self.validation_data = (np.array(val_data), )

    def on_epoch_end(self, epoch, logs):
        indices = np.random.randint(self.validation_data[0].shape[0], size=8)
        test_data = self.validation_data[0][indices]
        pred_data = np.clip(self.model.predict(test_data), 0, 1)
        wandb.log({
            "examples": [
                wandb.Image(np.hstack([data, pred_data[i]]), caption=str(i))
                for i, data in enumerate(test_data)]
        }, commit=False)
from keras.layers import Lambda, Input, Dense
from keras.models import Model
from keras.datasets import fashion_mnist
from keras.losses import mse, binary_crossentropy
from keras.utils import plot_model
from keras import backend as K
from plotutil import PlotCallback
import wandb
from wandb.keras import WandbCallback

import numpy as np
import os

wandb.init()
config = wandb.config

# reparameterization trick
# instead of sampling from Q(z|X), sample eps = N(0,I)
# z = z_mean + sqrt(var)*eps


def sampling(args):
    """Reparameterization trick by sampling fr an isotropic unit Gaussian.
    # Arguments:
        args (tensor): mean and log of variance of Q(z|X)
    # Returns:
        z (tensor): sampled latent vector
    """

    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    # by default, random_normal has mean=0 and std=1.0
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon


# MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

image_size = x_train.shape[1]
original_dim = image_size * image_size
x_train = np.reshape(x_train, [-1, original_dim])
x_test = np.reshape(x_test, [-1, original_dim])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# network parameters
input_shape = (original_dim, )
intermediate_dim = 512
batch_size = 128
latent_dim = 2
epochs = 50

# VAE model = encoder + decoder
# build encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

# use reparameterization trick to push the sampling out as input
z = Lambda(sampling, name='z')([z_mean, z_log_var])

# instantiate encoder model
encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')

# build decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = Dense(original_dim, activation='sigmoid')(x)

# instantiate decoder model
decoder = Model(latent_inputs, outputs, name='decoder')

# instantiate VAE model
outputs = decoder(encoder(inputs)[2])
vae = Model(inputs, outputs, name='vae_mlp')

models = (encoder, decoder)
data = (x_test, y_test)

reconstruction_loss = binary_crossentropy(inputs,
                                          outputs)

reconstruction_loss *= original_dim
kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')


vae.fit(x_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(x_test, None),
        callbacks=[WandbCallback(), PlotCallback(encoder, decoder, (x_test, y_test))])
vae.save_weights('vae_mlp_mnist.h5')
# get train.csv from https://www.kaggle.com/c/quora-question-pairs/data

from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout
from keras.preprocessing.sequence import pad_sequences
from keras.layers.advanced_activations import ELU
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras import backend as K
from keras.models import Model
from scipy import spatial
import tensorflow as tf
import pandas as pd
import numpy as np
import codecs
import csv
import os

import wandb

wandb.init()
config = wandb.config

config.train_data = 'small_train.csv'
config.glove_path = '../lstm/imdb-classifier/glove.6B.300d.txt'
VALIDATION_SPLIT = 0.2
MAX_SEQUENCE_LENGTH = 25
MAX_NB_WORDS = 20000
EMBEDDING_DIM = 300



texts = [] 
with codecs.open(config.train_data, encoding='utf-8') as f:
    reader = csv.reader(f, delimiter=',')
    header = next(reader)
    for values in reader:
        if len(values[3].split()) <= MAX_SEQUENCE_LENGTH:
            texts.append(values[3])
        if len(values[4].split()) <= MAX_SEQUENCE_LENGTH:
            texts.append(values[4])
print('Found %s texts in train.csv' % len(texts))
n_sents = len(texts)


#======================== Tokenize and pad texts lists ===================#
tokenizer = Tokenizer(MAX_NB_WORDS+1, oov_token='unk') #+1 for 'unk' token
tokenizer.fit_on_texts(texts)
print('Found %s unique tokens' % len(tokenizer.word_index))
## **Key Step** to make it work correctly otherwise drops OOV tokens anyway!
tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS} # <= because tokenizer is 1 indexed
tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1
word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding
index2word = {v: k for k, v in word_index.items()}
sequences = tokenizer.texts_to_sequences(texts)
data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', data_1.shape)
NB_WORDS = (min(tokenizer.num_words, len(word_index))+1) #+1 for zero padding 



#==================== sample train/validation data =====================#
data_val = data_1[:100]
data_train = data_1[100:]

#def sent_generator(TRAIN_DATA_FILE, chunksize):
#    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)
#    for df in reader:
#        #print(df.shape)
#        #df=pd.read_csv(TRAIN_DATA_FILE, iterator=False)
#        val3 = df.iloc[:,3:4].values.tolist()
#        val4 = df.iloc[:,4:5].values.tolist()
#        flat3 = [item for sublist in val3 for item in sublist]
#        flat4 = [str(item) for sublist in val4 for item in sublist]
#        texts = [] 
#        texts.extend(flat3[:])
#        texts.extend(flat4[:])
#        
#        sequences = tokenizer.texts_to_sequences(texts)
#        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
#        yield [data_train, data_train]





#======================== prepare GLOVE embeddings =============================#
embeddings_index = {}
f = open(config.glove_path, encoding='utf8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))
for word, i in word_index.items():
    if i < NB_WORDS+1: #+1 for 'unk' oov token
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            glove_embedding_matrix[i] = embedding_vector
        else:
            # words not found in embedding index will the word embedding of unk
            glove_embedding_matrix[i] = embeddings_index.get('unk')
print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))



#====================== VAE model ============================================#
batch_size = 100
max_len = MAX_SEQUENCE_LENGTH
emb_dim = EMBEDDING_DIM
latent_dim = 64
intermediate_dim = 256
epsilon_std = 1.0
kl_weight = 0.01
num_sampled=500
act = ELU()


x = Input(shape=(max_len,))
x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],
                            input_length=max_len, trainable=False)(x)
h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)
#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)
#h = Dropout(0.2)(h)
#h = Dense(intermediate_dim, activation='linear')(h)
#h = act(h)
#h = Dropout(0.2)(h)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,
                              stddev=epsilon_std)
    return z_mean + K.exp(z_log_var / 2) * epsilon

# note that "output_shape" isn't necessary with the TensorFlow backend
z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])
# we instantiate these layers separately so as to reuse them later
repeated_context = RepeatVector(max_len)
decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)
decoder_mean = Dense(NB_WORDS, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()
h_decoded = decoder_h(repeated_context(z))
x_decoded_mean = decoder_mean(h_decoded)


# placeholder loss
def zero_loss(y_true, y_pred):
    return K.zeros_like(y_pred)

# Custom loss layer
class CustomVariationalLayer(Layer):
    def __init__(self, **kwargs):
        self.is_placeholder = True
        super(CustomVariationalLayer, self).__init__(**kwargs)
        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)

    def vae_loss(self, x, x_decoded_mean):
        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)
        labels = tf.cast(x, tf.int32)
        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, 
                                                     weights=self.target_weights,
                                                     average_across_timesteps=False,
                                                     average_across_batch=False), axis=-1)#,
                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,
        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        xent_loss = K.mean(xent_loss)
        kl_loss = K.mean(kl_loss)
        return K.mean(xent_loss + kl_weight * kl_loss)

    def call(self, inputs):
        x = inputs[0]
        x_decoded_mean = inputs[1]
        print(x.shape, x_decoded_mean.shape)
        loss = self.vae_loss(x, x_decoded_mean)
        self.add_loss(loss, inputs=inputs)
        # we don't use this output, but it has to have the correct shape:
        return K.ones_like(x)
    
def kl_loss(x, x_decoded_mean):
    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    kl_loss = kl_weight * kl_loss
    return kl_loss

loss_layer = CustomVariationalLayer()([x, x_decoded_mean])
vae = Model(x, [loss_layer])
opt = Adam(lr=0.01) 
vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])
vae.summary()


#======================= Model training ==============================#
def create_model_checkpoint(dir, model_name):
    filepath = dir + '/' + model_name + ".h5" 
    directory = os.path.dirname(filepath)
    try:
        os.stat(directory)
    except:
        os.mkdir(directory)
    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)
    return checkpointer

checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')



vae.fit(data_train, data_train,
     shuffle=True,
     epochs=100,
     batch_size=batch_size)

print(K.eval(vae.optimizer.lr))
K.set_value(vae.optimizer.lr, 0.01)


vae.save('models/vae_lstm.h5')
#vae.load_weights('models/vae_seq2seq_test.h5')
# build a model to project inputs on the latent space
encoder = Model(x, z_mean)
#encoder.save('models/encoder32dim512hid30kvocab_loss29_val34.h5')

# build a generator that can sample from the learned distribution
decoder_input = Input(shape=(latent_dim,))
_h_decoded = decoder_h(repeated_context(decoder_input))
_x_decoded_mean = decoder_mean(_h_decoded)
_x_decoded_mean = Activation('softmax')(_x_decoded_mean)
generator = Model(decoder_input, _x_decoded_mean)


index2word = {v: k for k, v in word_index.items()}
index2word[0] = 'pad'

#test on a validation sentence
sent_idx = 100
sent_encoded = encoder.predict(data_val[sent_idx:sent_idx+2,:])
x_test_reconstructed = generator.predict(sent_encoded, batch_size = 1)
reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])
np.apply_along_axis(np.max, 1, x_test_reconstructed[0])
np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[0]))
word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))
print(' '.join(word_list))
original_sent = list(np.vectorize(index2word.get)(data_val[sent_idx]))
print(' '.join(original_sent))



#=================== Sentence processing and interpolation ======================#
# function to parse a sentence
def sent_parse(sentence, mat_shape):
    sequence = tokenizer.texts_to_sequences(sentence)
    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
    return padded_sent#[padded_sent, sent_one_hot]


# input: encoded sentence vector
# output: encoded sentence vector in dataset with highest cosine similarity
def find_similar_encoding(sent_vect):
    all_cosine = []
    for sent in sent_encoded:
        result = 1 - spatial.distance.cosine(sent_vect, sent)
        all_cosine.append(result)
    data_array = np.array(all_cosine)
    maximum = data_array.argsort()[-3:][::-1][1]
    new_vec = sent_encoded[maximum]
    return new_vec


# input: two points, integer n
# output: n equidistant points on the line between the input points (inclusive)
def shortest_homology(point_one, point_two, num):
    dist_vec = point_two - point_one
    sample = np.linspace(0, 1, num, endpoint = True)
    hom_sample = []
    for s in sample:
        hom_sample.append(point_one + s * dist_vec)
    return hom_sample



# input: original dimension sentence vector
# output: sentence text
def print_latent_sentence(sent_vect):
    sent_vect = np.reshape(sent_vect,[1,latent_dim])
    sent_reconstructed = generator.predict(sent_vect)
    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS])
    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)
    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))
    w_list = [w for w in word_list if w not in ['pad']]
    print(' '.join(w_list))
    #print(word_list)
    
       
        
def new_sents_interp(sent1, sent2, n):
    tok_sent1 = sent_parse(sent1, [27])
    tok_sent2 = sent_parse(sent2, [27])
    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)
    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)
    test_hom = shortest_homology(enc_sent1, enc_sent2, n)
    for point in test_hom:
        print_latent_sentence(point)
        

import tensorflow as tf
import wandb

# Set Hyper-parameters
wandb.init()
config = wandb.config
config.batch_size = 128
config.epochs = 10
config.learn_rate = 0.001
config.dropout = 0.3
config.dense_layer_nodes = 128

# Load data
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
num_classes = len(class_names)

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

# Convert class vectors to binary class matrices.
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Define model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same',
                                 input_shape=X_train.shape[1:], activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(config.dropout))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(config.dense_layer_nodes, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(config.learn_rate),
              metrics=['accuracy'])
# log the number of total parameters
config.total_params = model.count_params()
print("Total params: ", config.total_params)

model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=class_names, save_model=False)])
from keras.datasets import cifar10
from keras.models import load_model
from tests import test_pixel_removal
import numpy as np
import wandb

num_examples = 10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

labels=[
"Airplane",
"Automobile",
"Bird",
"Cat",
"Deer",
"Dog",
"Frog",
"Horse",
"Ship",
"Truck"

]

run = wandb.init(job_type='eval')
config = run.config
run.examples.set_columns((
    ('id', int),
    ('loss', float),
    ('label', str),
    ('prediction', str),
    ('accuracy', float),
    ('confidence', wandb.types.Percentage),
    ('predictions', wandb.types.Histogram),
    ('image', wandb.types.Image),
    ('importance-image', wandb.types.Image)
))

model = load_model(config.model)
pred = np.zeros(10)

accuracies = []
losses = []

for idx, (image, label) in enumerate(list(zip(x_test, y_test))[0:num_examples]):
    image = image.reshape(1, 32, 32, 3)
    baseline = model.predict([image])[0]
    prediction = np.argmax(baseline)

    label_confidence = baseline[label]


    print("Label", label)
    print("Predictions", baseline)

    logloss = max(-100, np.log(label_confidence))
    accuracy = 1. if baseline[label] > 0.5 else 0.

    accuracies.append(accuracy)
    losses.append(logloss)

    (image_path, importance_image_path) = test_pixel_removal(model, image, idx)
    image = open(image_path, 'rb').read()
    importance_image = open(importance_image_path, 'rb').read()

    for i in range(10):
        logpred = max(0, (np.log(baseline[i])+30.)/40.)
        pred[i] = logpred

    run.examples.add(
            {'id': idx,
             'loss': logloss,
             'label': labels[label[0]],
             'accuracy': accuracy,
             'prediction': labels[prediction],
             'confidence': label_confidence,
             'predictions': pred,
             'image': image,
             'importance-image': importance_image
             })


run.summary['acc'] = np.mean(accuracies)
run.summary['loss'] = np.mean(losses)
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import numpy as np
import os
import wandb
from wandb.keras import WandbCallback
import tensorflow as tf

run = wandb.init()
config = run.config
config.dropout = 0.25
config.dense_layer_nodes = 100
config.learn_rate = 0.08
config.batch_size = 128
config.epochs = 10

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
num_classes = len(class_names)

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Convert class vectors to binary class matrices.
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same',
                                 input_shape=X_train.shape[1:], activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(config.dropout))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(config.dense_layer_nodes, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])
# log the number of total parameters
config.total_params = model.count_params()
print("Total params: ", config.total_params)
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

datagen = ImageDataGenerator(width_shift_range=0.1)
datagen.fit(X_train)


# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(X_train, y_train,
                                 batch_size=config.batch_size),
                    steps_per_epoch=X_train.shape[0] // config.batch_size,
                    epochs=config.epochs,
                    validation_data=(X_test, y_test),
                    callbacks=[WandbCallback(data_type="image", labels=class_names)])
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
# Importing the ResNet50 model
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import numpy as np
import os
import wandb
from wandb.keras import WandbCallback

# Set hyper parameters
wandb.init()
config = wandb.config
config.batch_size = 128
config.epochs = 10
config.learn_rate = 0.001
config.dropout = 0.3
config.dense_layer_nodes = 128

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
num_classes = len(class_names)

# Load and normalize data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

# Convert class vectors to binary class matrices.
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Loading the ResNet50 model with pre-trained ImageNet weights
big_model = ResNet50(weights='imagenet', include_top=False,
                     input_shape=(X_train.shape[1], X_train.shape[2], 3))

# Add new last layer
model = tf.keras.models.Sequential()
model.add(big_model)
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(10, activation='softmax'))

# Make the big first layer frozen for speed
model.layers[0].trainable = False

model.compile(loss='categorical_crossentropy',
              optimizer=Adam(config.learn_rate),
              metrics=['accuracy'])
# log the number of total parameters
config.total_params = model.count_params()
print("Total params: ", config.total_params)
model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test),
          callbacks=[WandbCallback(data_type="image", labels=class_names, save_model=False)])
import tensorflow as tf
import wandb

wandb.init()

config = wandb.config
config.batch_size = 128
config.epochs = 10
config.learn_rate = 1000

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
num_classes = len(class_names)

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Convert class vectors to binary class matrices.
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=X_train.shape[1:]))
model.add(tf.keras.layers.Dense(num_classes))
model.compile(loss='mse',
              optimizer=tf.keras.optimizers.Adam(config.learn_rate),
              metrics=['accuracy'])
# log the number of total parameters
config.total_params = model.count_params()
print("Total params: ", config.total_params)
model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=class_names, save_model=False)])
import tensorflow as tf
import wandb

# initialize wandb & set hyperparamers
run = wandb.init()
config = run.config
config.first_layer_convs = 32
config.first_layer_conv_width = 3
config.first_layer_conv_height = 3
config.dropout = 0.2
config.dense_layer_size = 128
config.img_width = 28
config.img_height = 28
config.epochs = 4

# load and normalize data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# reshape input data
X_train = X_train.reshape(
    X_train.shape[0], config.img_width, config.img_height, 1)
X_test = X_test.reshape(
    X_test.shape[0], config.img_width, config.img_height, 1)

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
num_classes = y_test.shape[1]
labels = [str(i) for i in range(10)]

# build model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32,
                                 (config.first_layer_conv_width,
                                  config.first_layer_conv_height),
                                 input_shape=(28, 28, 1),
                                 activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(config.dense_layer_size, activation='relu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'], weighted_metrics=['accuracy'])

model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=config.epochs,
          callbacks=[wandb.keras.WandbCallback(data_type="image", save_model=False)])
# This does a convolution on an image
# if you get an error
# ImportError: No module named skimage
# you may need to run:
# > pip install scikit-image
# > pip install scipy

import numpy as np

from skimage import io
from skimage import data
from scipy.signal import convolve2d

image = io.imread('dog.jpg', as_grey=True)

kernel = [[0.0, 0.0, 0.0],
          [0.0, 1.0, 0.0],
          [0.0, 0.0, 0.0]]

new_image = convolve2d(image, kernel).clip(0.0, 1.0)

io.imsave('out.png', new_image)
import numpy as np

from skimage import io
from skimage import data
from skimage.measure import block_reduce

image = io.imread('dog.jpg', as_grey=True)

new_image = block_reduce(image, block_size=(5, 5), func=np.max)

io.imsave('out.png', new_image)
from keras.datasets import mnist
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Input, Add
from keras.utils import np_utils
from wandb.keras import WandbCallback
import wandb
import os

run = wandb.init()
config = run.config
config.first_layer_convs = 32
config.first_layer_conv_width = 3
config.first_layer_conv_height = 3
config.dropout = 0.2
config.dense_layer_size = 128
config.img_width = 28
config.img_height = 28
config.epochs = 10

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# reshape input data
X_train = X_train.reshape(
    X_train.shape[0], config.img_width, config.img_height, 1)
X_test = X_test.reshape(
    X_test.shape[0], config.img_width, config.img_height, 1)

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
labels = [str(i) for i in range(10)]

# build model
input = Input(shape=(28, 28, 1))
input_copy = input
conv_out = Conv2D(32,
                  (config.first_layer_conv_width, config.first_layer_conv_height),
                  activation='relu', padding='same')(input)
res_input = Conv2D(32, (1, 1), activation='relu', padding='same')(input)
add_out = Add()([conv_out, res_input])

max_pool_out = MaxPooling2D(pool_size=(2, 2))(conv_out)
flatten_out = Flatten()(max_pool_out)
dense1_out = Dense(config.dense_layer_size, activation='relu')(flatten_out)
dense2_out = Dense(num_classes, activation='softmax')(dense1_out)

model = Model(input, dense2_out)


model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])


model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=config.epochs,
          callbacks=[WandbCallback(data_type="image", save_model=False)])
from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, UpSampling2D, MaxPooling2D
from keras.models import Model, Sequential
from keras.datasets import mnist
from keras.callbacks import Callback
import random
import glob
import wandb
from wandb.keras import WandbCallback
import subprocess
import os

from PIL import Image
import numpy as np

run = wandb.init()
config = run.config

config.num_epochs = 100
config.batch_size = 4
config.img_dir = "images"
config.height = 256
config.width = 256

val_dir = 'dogcat-data/validation/cat'
train_dir = 'dogcat-data/train/cat'
if not os.path.exists("dogcat-data"):
      if os.path.exists("../keras-transfer/dogcat-data"):
            subprocess.check_output("cp -r ../keras-transfer/dogcat-data .", shell=True)
      else:
            print("Downloading dog/cat dataset...")
            subprocess.check_output("curl https://storage.googleapis.com/wandb-production.appspot.com/qualcomm/dogcat-data.tgz | tar xvz", shell=True)
            subprocess.check_output("rm dogcat-data/train/dog/._dog* dogcat-data/train/cat/._cat* dogcat-data/validation/cat/._cat* dogcat-data/validation/dog/._dog*", shell=True)


def my_generator(batch_size, img_dir):
      image_filenames = glob.glob(img_dir + "/*")
      counter = 0
      while True:
            bw_images = np.zeros((batch_size, config.width, config.height))
            color_images = np.zeros((batch_size, config.width, config.height, 3))
            random.shuffle(image_filenames) 
            if ((counter+1)*batch_size>=len(image_filenames)):
                  counter = 0
            for i in range(batch_size):
                  img = Image.open(image_filenames[counter + i]).resize((config.width, config.height))
                  color_images[i] = np.array(img)
                  bw_images[i] = np.array(img.convert('L'))
            yield (bw_images, color_images)
            counter += batch_size 

model = Sequential()
model.add(Reshape((config.height,config.width,1), input_shape=(config.height,config.width)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(3, (3, 3), activation='relu', padding='same'))


model.compile(optimizer='adam', loss='mse')

model.summary()

(val_bw_images, val_color_images) = next(my_generator(20, val_dir))

model.fit_generator( my_generator(config.batch_size, train_dir),
                     samples_per_epoch=20,
                     nb_epoch=config.num_epochs, callbacks=[WandbCallback(data_type='image', save_model=False)],
                     validation_data=(val_bw_images, val_color_images))

from keras.preprocessing.image import load_img, img_to_array
import numpy as np
import scipy

from keras.applications import inception_v3
from keras import backend as K

base_image_path = "elephant.jpg"
result_prefix = "output"

# Playing with these hyperparameters will also allow you to achieve new effects
step = 0.01  # Gradient ascent step size
num_octave = 3  # Number of scales at which to run gradient ascent
octave_scale = 1.4  # Size ratio between scales
iterations = 20  # Number of ascent steps per scale
max_loss = 10.

settings = {
    'features': {
        #'mixed1':1.0
        #'mixed2': 0.2,
        #'mixed3': 0.5,
        'mixed4': 2.,
        #'mixed8': 1.5,
    },
}


def preprocess_image(image_path):
    # Util function to open, resize and format pictures
    # into appropriate tensors.
    img = load_img(image_path)
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = inception_v3.preprocess_input(img)
    return img


def deprocess_image(x):
    # Util function to convert a tensor into a valid image.
    x = x.reshape((x.shape[1], x.shape[2], 3))
    x /= 2.
    x += 0.5
    x *= 255.
    x = np.clip(x, 0, 255).astype('uint8')
    return x

K.set_learning_phase(0)

# Build the InceptionV3 network with our placeholder.
# The model will be loaded with pre-trained ImageNet weights.
model = inception_v3.InceptionV3(weights='imagenet',
                                 include_top=False)
dream = model.input
model.summary()
print('Model loaded.')

# Get the symbolic outputs of each "key" layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers])

# Define the loss.
loss = K.variable(0.)
for layer_name in settings['features']:
    # Add the L2 norm of the features of a layer to the loss.
    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'
    coeff = settings['features'][layer_name]
    x = layer_dict[layer_name].output
    # We avoid border artifacts by only involving non-border pixels in the loss.
    scaling = K.prod(K.cast(K.shape(x), 'float32'))

    loss += coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling

# Compute the gradients of the dream wrt the loss.
grads = K.gradients(loss, dream)[0]
# Normalize gradients.
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)

# Set up function to retrieve the value
# of the loss and gradients given an input image.
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)


def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values


def resize_img(img, size):
    img = np.copy(img)

    factors = (1,
               float(size[0]) / img.shape[1],
               float(size[1]) / img.shape[2],
               1)
    return scipy.ndimage.zoom(img, factors, order=1)


def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss:
            break
        print('..Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x


def save_img(img, fname):
    pil_img = deprocess_image(np.copy(img))
    scipy.misc.imsave(fname, pil_img)


"""Process:
- Load the original image.
- Define a number of processing scales (i.e. image shapes),
    from smallest to largest.
- Resize the original image to the smallest scale.
- For every scale, starting with the smallest (i.e. current one):
    - Run gradient ascent
    - Upscale image to the next scale
    - Reinject the detail that was lost at upscaling time
- Stop when we are back to the original size.
To obtain the detail lost during upscaling, we simply
take the original image, shrink it down, upscale it,
and compare the result to the (resized) original image.
"""

img = preprocess_image(base_image_path)

original_shape = img.shape[1:3]
successive_shapes = [original_shape]
for i in range(1, num_octave):
    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])
    successive_shapes.append(shape)
successive_shapes = successive_shapes[::-1]
original_img = np.copy(img)
shrunk_original_img = resize_img(img, successive_shapes[0])

for i, shape in enumerate(successive_shapes):
    print('Processing image shape', shape)
    img = resize_img(img, shape)
    img = gradient_ascent(img,
                          iterations=iterations,
                          step=step,
                          max_loss=max_loss)
    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)
    same_size_original = resize_img(original_img, shape)
    lost_detail = same_size_original - upscaled_shrunk_original_img

    img += lost_detail
    shrunk_original_img = resize_img(original_img, shape)

    save_img(img, fname=result_prefix + str(i) + '.png')
import pandas as pd
import numpy as np
from keras.utils import to_categorical

import wandb
from wandb.wandb_keras import WandbKerasCallback
run = wandb.init()
config = run.config

df = pd.read_csv('../scikit/tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

# mapping from labels to numbers
mapping = {'Negative emotion':0,
    'No emotion toward brand or product':1,
    'Positive emotion':2,
    'I can\'t tell':3}
numeric_target = [mapping[t] for t in fixed_target]
num_labels = len(mapping)

# one hot encode outputs
labels = to_categorical(numeric_target)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=config.max_words)
tokenizer.fit_on_texts(fixed_text)
sequences = tokenizer.texts_to_sequences(fixed_text)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=config.max_sequence_length)

print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(config.validation_split * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

# Load the word embedding
embeddings_index = {}
f = open('../scikit/glove/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

embedding_dim = 100
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

from keras.layers import Embedding, Input, Dense, Flatten, Conv1D
from keras.layers import MaxPooling1D, Dropout
from keras.models import Model

embedding_layer = Embedding(len(word_index) + 1,
                            embedding_dim,
                            weights=[embedding_matrix],
                            input_length=config.max_sequence_length,
                            trainable=False)

sequence_input = Input(shape=(config.max_sequence_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Dropout(0.3)(embedded_sequences)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Dropout(0.3)(x)
#x = Conv1D(128, 5, activation='relu')(x)
#x = MaxPooling1D(5)(x)  # global max pooling
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
preds = Dense(num_labels, activation='softmax')(x)

model = Model(sequence_input, preds)
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=config.epochs, batch_size=config.batch_size,
          callbacks=[WandbKerasCallback()])
import tensorflow as tf
import wandb

# logging code
run = wandb.init()
config = run.config
config.epochs = 100
config.lr = 0.01
config.layers = 3
config.dropout = 0.4
config.hidden_layer_1_size = 128

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

img_width = X_train.shape[1]
img_height = X_train.shape[2]
labels = ["T-shirt/top", "Trouser", "Pullover", "Dress",
          "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28)))
model.add(tf.keras.layers.Dense(num_classes))
model.compile(loss='mse', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=config.epochs, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels)])

'''
DCGAN on MNIST using Keras
Author: Rowel Atienza
Project: https://github.com/roatienza/Deep-Learning-Experiments
Dependencies: tensorflow 1.0 and keras 2.0
If you get an error about _tk_inter, run: sudo apt-get install python3.6-tk
Usage: python3 dcgan_mnist.py
'''

import numpy as np
import time
from tensorflow.examples.tutorials.mnist import input_data

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Reshape
from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D
from keras.layers import LeakyReLU, Dropout
from keras.layers import BatchNormalization
from keras.optimizers import Adam, RMSprop

import matplotlib.pyplot as plt

class ElapsedTimer(object):
    def __init__(self):
        self.start_time = time.time()
    def elapsed(self,sec):
        if sec < 60:
            return str(sec) + " sec"
        elif sec < (60 * 60):
            return str(sec / 60) + " min"
        else:
            return str(sec / (60 * 60)) + " hr"
    def elapsed_time(self):
        print("Elapsed: %s " % self.elapsed(time.time() - self.start_time) )

class DCGAN(object):
    def __init__(self, img_rows=28, img_cols=28, channel=1):

        self.img_rows = img_rows
        self.img_cols = img_cols
        self.channel = channel
        self.D = None   # discriminator
        self.G = None   # generator
        self.AM = None  # adversarial model
        self.DM = None  # discriminator model

    def discriminator(self):
        if self.D:
            return self.D
        self.D = Sequential()
        depth = 64
        dropout = 0.4
        # In: 28 x 28 x 1, depth = 1
        # Out: 14 x 14 x 1, depth=64
        input_shape = (self.img_rows, self.img_cols, self.channel)
        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\
            padding='same'))
        self.D.add(LeakyReLU(alpha=0.2))
        self.D.add(Dropout(dropout))

        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))
        self.D.add(LeakyReLU(alpha=0.2))
        self.D.add(Dropout(dropout))

        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))
        self.D.add(LeakyReLU(alpha=0.2))
        self.D.add(Dropout(dropout))

        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))
        self.D.add(LeakyReLU(alpha=0.2))
        self.D.add(Dropout(dropout))

        # Out: 1-dim probability
        self.D.add(Flatten())
        self.D.add(Dense(1))
        self.D.add(Activation('sigmoid'))
        self.D.summary()
        return self.D

    def generator(self):
        if self.G:
            return self.G
        self.G = Sequential()
        dropout = 0.4
        depth = 64+64+64+64
        dim = 7
        # In: 100
        # Out: dim x dim x depth
        self.G.add(Dense(dim*dim*depth, input_dim=100))
        self.G.add(BatchNormalization(momentum=0.9))
        self.G.add(Activation('relu'))
        self.G.add(Reshape((dim, dim, depth)))
        self.G.add(Dropout(dropout))

        # In: dim x dim x depth
        # Out: 2*dim x 2*dim x depth/2
        self.G.add(UpSampling2D())
        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))
        self.G.add(BatchNormalization(momentum=0.9))
        self.G.add(Activation('relu'))

        self.G.add(UpSampling2D())
        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))
        self.G.add(BatchNormalization(momentum=0.9))
        self.G.add(Activation('relu'))

        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))
        self.G.add(BatchNormalization(momentum=0.9))
        self.G.add(Activation('relu'))

        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix
        self.G.add(Conv2DTranspose(1, 5, padding='same'))
        self.G.add(Activation('sigmoid'))
        self.G.summary()
        return self.G

    def discriminator_model(self):
        if self.DM:
            return self.DM
        optimizer = RMSprop(lr=0.0002, decay=6e-8)
        self.DM = Sequential()
        self.DM.add(self.discriminator())
        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\
            metrics=['accuracy'])
        return self.DM

    def adversarial_model(self):
        if self.AM:
            return self.AM
        optimizer = RMSprop(lr=0.0001, decay=3e-8)
        self.AM = Sequential()
        self.AM.add(self.generator())
        self.AM.add(self.discriminator())
        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\
            metrics=['accuracy'])
        return self.AM

class MNIST_DCGAN(object):
    def __init__(self):
        self.img_rows = 28
        self.img_cols = 28
        self.channel = 1

        self.x_train = input_data.read_data_sets("mnist",\
        	one_hot=True).train.images
        self.x_train = self.x_train.reshape(-1, self.img_rows,\
        	self.img_cols, 1).astype(np.float32)

        self.DCGAN = DCGAN()
        self.discriminator =  self.DCGAN.discriminator_model()
        self.adversarial = self.DCGAN.adversarial_model()
        self.generator = self.DCGAN.generator()

    def train(self, train_steps=2000, batch_size=256, save_interval=0):
        noise_input = None
        if save_interval>0:
            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])
        for i in range(train_steps):
            images_train = self.x_train[np.random.randint(0,
                self.x_train.shape[0], size=batch_size), :, :, :]
            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])
            images_fake = self.generator.predict(noise)
            x = np.concatenate((images_train, images_fake))
            y = np.ones([2*batch_size, 1])
            y[batch_size:, :] = 0
            d_loss = self.discriminator.train_on_batch(x, y)

            y = np.ones([batch_size, 1])
            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])
            a_loss = self.adversarial.train_on_batch(noise, y)
            log_mesg = "%d: [D loss: %f, acc: %f]" % (i, d_loss[0], d_loss[1])
            log_mesg = "%s  [A loss: %f, acc: %f]" % (log_mesg, a_loss[0], a_loss[1])
            print(log_mesg)
            if save_interval>0:
                if (i+1)%save_interval==0:
                    self.plot_images(save2file=True, samples=noise_input.shape[0],\
                        noise=noise_input, step=(i+1))

    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):
        filename = 'mnist.png'
        if fake:
            if noise is None:
                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])
            else:
                filename = "mnist_%d.png" % step
            images = self.generator.predict(noise)
        else:
            i = np.random.randint(0, self.x_train.shape[0], samples)
            images = self.x_train[i, :, :, :]

        plt.figure(figsize=(10,10))
        for i in range(images.shape[0]):
            plt.subplot(4, 4, i+1)
            image = images[i, :, :, :]
            image = np.reshape(image, [self.img_rows, self.img_cols])
            plt.imshow(image, cmap='gray')
            plt.axis('off')
        plt.tight_layout()
        if save2file:
            plt.savefig(filename)
            plt.close('all')
        else:
            plt.show()

if __name__ == '__main__':
    mnist_dcgan = MNIST_DCGAN()
    timer = ElapsedTimer()
    mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)
    timer.elapsed_time()
    mnist_dcgan.plot_images(fake=True)
    mnist_dcgan.plot_images(fake=False, save2file=True)
import os
import numpy as np

from keras.layers import Input
from keras.models import Model, Sequential
from keras.layers.core import Reshape, Dense, Dropout, Flatten
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import Convolution2D, UpSampling2D
from keras.layers.normalization import BatchNormalization
from keras.datasets import mnist
from keras.optimizers import Adam
from keras import backend as K
from keras import initializers
from PIL import Image
from keras.callbacks import LambdaCallback
import wandb

# Find more tricks here: https://github.com/soumith/ganhacks

run = wandb.init()
config = wandb.config

# The results are a little better when the dimensionality of the random vector is only 10.
# The dimensionality has been left at 100 for consistency with other GAN implementations.
randomDim = 10

# Load MNIST data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = (X_train.astype(np.float32) - 127.5)/127.5
X_train = X_train.reshape(60000, 784)

config.lr=0.0002
config.beta_1=0.5
config.batch_size=128
config.epochs=10

# Optimizer
adam = Adam(config.lr, beta_1=config.beta_1)

generator = Sequential()
generator.add(Dense(256, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))
generator.add(LeakyReLU(0.2))
generator.add(Dense(512))
generator.add(LeakyReLU(0.2))
generator.add(Dense(1024))
generator.add(LeakyReLU(0.2))
generator.add(Dense(784, activation='tanh'))
generator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])

discriminator = Sequential()
discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(512))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(256))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(1, activation='sigmoid'))
discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['binary_accuracy'])

# Combined network
discriminator.trainable = False
ganInput = Input(shape=(randomDim,))
x = generator(ganInput)
ganOutput = discriminator(x)
gan = Model(inputs=ganInput, outputs=ganOutput)
gan.compile(loss='binary_crossentropy', optimizer=adam, metrics = ['binary_accuracy'])

iter = 0
# Write out generated MNIST images
def writeGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):
    noise = np.random.normal(0, 1, size=[examples, randomDim])
    generatedImages = generator.predict(noise)
    generatedImages = generatedImages.reshape(examples, 28, 28)
    
    imgs = [None] * 10
    for i in range(10):
        imgs[i] = Image.fromarray((generatedImages[0] + 1.)* (255/2.))
        imgs[i] = imgs[i].convert('RGB')
        imgs[i] = (wandb.Image(imgs[i]))
    wandb.log({"image": imgs}, commit=False)


# Save the generator and discriminator networks (and weights) for later use
def saveModels(epoch):
    generator.save('models/gan_generator_epoch_%d.h5' % epoch)
    discriminator.save('models/gan_discriminator_epoch_%d.h5' % epoch)


def log_generator(epoch, logs):
    global iter
    iter += 1
    if iter % 500 == 0:
        wandb.log({'generator_loss': logs['loss'],
                     'generator_acc': logs['binary_accuracy'],
                     'discriminator_loss': 0.0,
                     'discriminator_acc': (1-logs['binary_accuracy'])})

def log_discriminator(epoch, logs):
    global iter
    if iter % 500 == 250:
        wandb.log({
            'generator_loss': 0.0,
            'generator_acc': logs['binary_accuracy'],
            'discriminator_loss': logs['loss'],
            'discriminator_acc': logs['binary_accuracy']})

def train(epochs=config.epochs, batchSize=config.batch_size):
    batchCount = int(X_train.shape[0] / config.batch_size)
    print('Epochs:', epochs)
    print('Batch size:', batchSize)
    print('Batches per epoch:', batchCount)

    wandb_logging_callback_d = LambdaCallback(on_epoch_end=log_discriminator)
    wandb_logging_callback_g = LambdaCallback(on_epoch_end=log_generator)


    for e in range(1, epochs+1):
        print("Epoch {}:".format(e))
        for i in range(batchCount):
            # Get a random set of input noise and images
            noise = np.random.normal(0, 1, size=[batchSize, randomDim])
            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]

            # Generate fake MNIST images
            generatedImages = generator.predict(noise)
            # print np.shape(imageBatch), np.shape(generatedImages)
            X = np.concatenate([imageBatch, generatedImages])

            # Labels for generated and real data
            yDis = np.zeros(2*batchSize)
            # One-sided label smoothing
            yDis[:batchSize] = 0.9

            # Train discriminator
            discriminator.trainable = True
            dloss = discriminator.fit(X, yDis, verbose=0, callbacks=[wandb_logging_callback_d])

            # Train generator
            noise = np.random.normal(0, 1, size=[batchSize, randomDim])
            yGen = np.ones(batchSize)
            discriminator.trainable = False
            gloss = gan.fit(noise, yGen, verbose=0, callbacks=[wandb_logging_callback_g])

            writeGeneratedImages(i)

        print("Discriminator loss: {}, acc: {}".format(dloss.history["loss"][-1], dloss.history["binary_accuracy"][-1]))
        print("Generator loss: {}, acc: {}".format(gloss.history["loss"][-1], 1-gloss.history["binary_accuracy"][-1]))


if __name__ == '__main__':
    train(200, 128)
from __future__ import print_function, division
import matplotlib
matplotlib.use('agg')  # noqa
import numpy as np
import sys
import matplotlib.pyplot as plt
import wandb
import tensorflow as tf

run = wandb.init()
config = wandb.config


class GAN():
    def __init__(self):
        self.img_rows = 28
        self.img_cols = 28
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100

        optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)

        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(loss='binary_crossentropy',
                                   optimizer=optimizer,
                                   metrics=['accuracy'])

        # Build the generator
        self.generator = self.build_generator()

        # The generator takes noise as input and generates imgs
        z = tf.keras.layers.Input(shape=(self.latent_dim,))
        img = self.generator(z)

        # For the combined model we will only train the generator
        self.discriminator.trainable = False

        # The discriminator takes generated images as input and determines validity
        validity = self.discriminator(img)

        # The combined model  (stacked generator and discriminator)
        # Trains the generator to fool the discriminator
        self.combined = tf.keras.models.Model(z, validity)
        wandb.run.summary['graph'] = wandb.Graph.from_keras(self.combined)
        wandb.run._user_accessed_summary = False
        self.combined.summary()
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    def build_generator(self):
        """The model that generates imagery from a latent vector"""
        model = tf.keras.models.Sequential()

        model.add(tf.keras.layers.Dense(256, input_dim=self.latent_dim))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.BatchNormalization(momentum=0.8))
        model.add(tf.keras.layers.Dense(512))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.BatchNormalization(momentum=0.8))
        model.add(tf.keras.layers.Dense(1024))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.BatchNormalization(momentum=0.8))
        model.add(tf.keras.layers.Dense(
            np.prod(self.img_shape), activation='tanh'))
        model.add(tf.keras.layers.Reshape(self.img_shape))

        model.summary()

        noise = tf.keras.layers.Input(shape=(self.latent_dim,))
        img = model(noise)

        return tf.keras.models.Model(noise, img)

    def build_discriminator(self):
        """The model that classifies images as real or fake"""
        model = tf.keras.models.Sequential()

        model.add(tf.keras.layers.Flatten(input_shape=self.img_shape))
        model.add(tf.keras.layers.Dense(512))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.Dense(256))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
        model.summary()

        img = tf.keras.layers.Input(shape=self.img_shape)
        validity = model(img)

        return tf.keras.models.Model(img, validity)

    def train(self, epochs, batch_size=128, sample_interval=50):

        # Load the dataset
        (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()

        # Rescale -1 to 1
        X_train = X_train / 127.5 - 1.
        X_train = np.expand_dims(X_train, axis=3)

        # Adversarial ground truths
        valid = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))

        for epoch in range(epochs):

            # ---------------------
            #  Train Discriminator
            # ---------------------

            # Select a random batch of images
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            imgs = X_train[idx]

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Generate a batch of new images
            gen_imgs = self.generator.predict(noise)

            # Train the discriminator
            d_loss_real = self.discriminator.train_on_batch(imgs, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            #  Train Generator
            # ---------------------

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Train the generator (to have the discriminator label samples as valid)
            g_loss = self.combined.train_on_batch(noise, valid)

            # If at save interval => save generated image samples
            if epoch % sample_interval == 0:
                print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" %
                      (epoch, d_loss[0], 100*d_loss[1], g_loss))
                wandb.log({
                    "discriminator_acc": 100*d_loss[1],
                    "discriminator_loss": d_loss[0],
                    "epoch": epoch,
                    "generator_loss": g_loss,
                    "examples": [wandb.Image(img) for img in gen_imgs]
                })
                self.sample_images(epoch)

    def sample_images(self, epoch):
        r, c = 5, 5
        noise = np.random.normal(0, 1, (r * c, self.latent_dim))
        gen_imgs = self.generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')
                axs[i, j].axis('off')
                cnt += 1
        fig.savefig("latest_example.png")  # % epoch)
        plt.close()


if __name__ == '__main__':
    gan = GAN()
    gan.train(epochs=300000, batch_size=32, sample_interval=500)
# curl http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
# tar xvfz aclImdb_v1.tar.gz
import imdb
import numpy as np
from keras.preprocessing import text
import wandb
from sklearn.linear_model import LogisticRegression

wandb.init()
config = wandb.config
config.vocab_size = 1000

(X_train, y_train), (X_test, y_test) = imdb.load_imdb()


tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)
X_test = tokenizer.texts_to_matrix(X_test)

bow_model = LogisticRegression()
bow_model.fit(X_train, y_train)

pred_train = bow_model.predict(X_train)
acc = np.sum(pred_train==y_train)/len(pred_train)

pred_test = bow_model.predict(X_test)
val_acc = np.sum(pred_test==y_test)/len(pred_test)
wandb.log({"val_acc": val_acc, "acc": acc})from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM
from keras.layers import Conv1D, Flatten
from keras.datasets import imdb
import wandb
from wandb.keras import WandbCallback
import imdb
import numpy as np
from keras.preprocessing import text

wandb.init()
config = wandb.config

# set parameters:
config.vocab_size = 1000
config.maxlen = 1000
config.batch_size = 32
config.embedding_dims = 50
config.filters = 250
config.kernel_size = 3
config.hidden_dims = 250
config.epochs = 10

(X_train, y_train), (X_test, y_test) = imdb.load_imdb()

tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)
X_test = tokenizer.texts_to_matrix(X_test)

X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)

model = Sequential()
model.add(Embedding(config.vocab_size,
                    config.embedding_dims,
                    input_length=config.maxlen))
model.add(Dropout(0.5))
model.add(Conv1D(config.filters,
                 config.kernel_size,
                 padding='valid',
                 activation='relu'))
model.add(Flatten())
model.add(Dense(config.hidden_dims, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(X_train, y_train,
          batch_size=config.batch_size,
          epochs=config.epochs,
          validation_data=(X_test, y_test), callbacks=[WandbCallback()])
# need to download glove from http://nlp.stanford.edu/data/glove.6B.zip
# wget http://nlp.stanford.edu/data/glove.6B.zip
# unzip http://nlp.stanford.edu/data/glove.6B.zip

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM
from keras.layers import Conv1D, Flatten
from keras.datasets import imdb
import wandb
from wandb.keras import WandbCallback
import imdb
import numpy as np
from keras.preprocessing import text

wandb.init()
config = wandb.config

# set parameters:
config.vocab_size = 1000
config.maxlen = 300
config.batch_size = 32
config.embedding_dims = 50
config.filters = 250
config.kernel_size = 3
config.hidden_dims = 100
config.epochs = 10

(X_train, y_train), (X_test, y_test) = imdb.load_imdb()

tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)
X_test = tokenizer.texts_to_matrix(X_test)

X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)

embeddings_index = dict()
f = open('glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

embedding_matrix = np.zeros((config.vocab_size, 100))
for word, index in tokenizer.word_index.items():
    if index > config.vocab_size - 1:
        break
    else:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector


## create model
model = Sequential()
model.add(Embedding(config.vocab_size, 100, input_length=config.maxlen, weights=[embedding_matrix], trainable=False))
model.add(LSTM(config.hidden_dims, activation="sigmoid"))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(X_train, y_train,
          batch_size=config.batch_size,
          epochs=config.epochs,
          validation_data=(X_test, y_test), callbacks=[WandbCallback()])




from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM
from keras.layers import Conv1D, Flatten
from keras.datasets import imdb
import wandb
from wandb.keras import WandbCallback
import imdb
import numpy as np
from keras.preprocessing import text

wandb.init()
config = wandb.config

# set parameters:
config.vocab_size = 1000
config.maxlen = 300
config.batch_size = 32
config.embedding_dims = 50
config.filters = 250
config.kernel_size = 3
config.hidden_dims = 100
config.epochs = 10

(X_train, y_train), (X_test, y_test) = imdb.load_imdb()

tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)
X_test = tokenizer.texts_to_matrix(X_test)

X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)

model = Sequential()
model.add(Embedding(config.vocab_size,
                    config.embedding_dims,
                    input_length=config.maxlen))
model.add(LSTM(config.hidden_dims, activation="sigmoid"))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(X_train, y_train,
          batch_size=config.batch_size,
          epochs=config.epochs,
          validation_data=(X_test, y_test), callbacks=[WandbCallback()])
import os

def load_imdb():
    X_train = []
    y_train = []

    path = './aclImdb/train/pos/'
    X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])
    y_train.extend([1 for _ in range(12500)])

    path = './aclImdb/train/neg/'
    X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])
    y_train.extend([0 for _ in range(12500)])

    X_test = []
    y_test = []
    
    path = './aclImdb/test/pos/'
    X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])
    y_test.extend([1 for _ in range(12500)])

    path = './aclImdb/test/neg/'
    X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])
    y_test.extend([0 for _ in range(12500)])
    
    return (X_train, y_train), (X_test, y_test)import tensorflow as tf
import wandb

run = wandb.init()
config = run.config
config.optimizer = "adam"
config.epochs = 50
config.dropout = 0.4
config.hidden_nodes = 100

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# normalize data
X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]
num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(config.hidden_nodes, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=config.optimizer,
              metrics=['accuracy'])


# Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=config.epochs, callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels)])
import tensorflow as tf
import wandb

# initialize wandb & set hyperparamers
run = wandb.init()
config = run.config
config.optimizer = "adam"
config.epochs = 50
config.dropout = 10
config.hidden_nodes = 100

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# normalize data
X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]
num_classes = y_train.shape[1]


# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(config.hidden_nodes, activation='relu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=config.optimizer,
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=config.epochs,
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels)])
# Adapted from https://github.com/pytorch/vision/blob/master/torchvision/datasets/omniglot.py


from PIL import Image
from os.path import join
import os
import torch.utils.data as data
from .utils import download_url, check_integrity, list_dir, list_files


class Omniglot(data.Dataset):
    """`Omniglot <https://github.com/brendenlake/omniglot>`_ Dataset.
    Args:
        root (string): Root directory of dataset where directory
            ``omniglot-py`` exists.
        background (bool, optional): If True, creates dataset from the "background" set, otherwise
            creates from the "evaluation" set. This terminology is defined by the authors.
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
        download (bool, optional): If true, downloads the dataset zip files from the internet and
            puts it in root directory. If the zip files are already downloaded, they are not
            downloaded again.
    """
    folder = 'omniglot-py'
    download_url_prefix = 'https://github.com/brendenlake/omniglot/raw/master/python'
    zips_md5 = {
        'images_background': '68d2efa1b9178cc56df9314c21c6e718',
        'images_evaluation': '6b91aef0f799c5bb55b94e3f2daec811'
    }

    def __init__(self, root, background=True,
                 transform=None, target_transform=None,
                 download=False):
        self.root = join(os.path.expanduser(root), self.folder)
        self.background = background
        self.transform = transform
        self.target_transform = target_transform

        if download:
            self.download()

        if not self._check_integrity():
            raise RuntimeError('Dataset not found or corrupted.' +
                               ' You can use download=True to download it')

        self.target_folder = join(self.root, self._get_target_folder())
        self._alphabets = list_dir(self.target_folder)
        self._characters = sum([[join(a, c) for c in list_dir(join(self.target_folder, a))]
                                for a in self._alphabets], [])
        self._character_images = [[(image, idx) for image in list_files(join(self.target_folder, character), '.png')]
                                  for idx, character in enumerate(self._characters)]
        self._flat_character_images = sum(self._character_images, [])

    def __len__(self):
        return len(self._flat_character_images)

    def __getitem__(self, index):
        """
        Args:
            index (int): Index
        Returns:
            tuple: (image, target) where target is index of the target character class.
        """
        image_name, character_class = self._flat_character_images[index]
        image_path = join(self.target_folder, self._characters[character_class], image_name)
        image = Image.open(image_path, mode='r').convert('L')

        if self.transform:
            image = self.transform(image)

        if self.target_transform:
            character_class = self.target_transform(character_class)

        return image, character_class

    def _check_integrity(self):
        zip_filename = self._get_target_folder()
        if not check_integrity(join(self.root, zip_filename + '.zip'), self.zips_md5[zip_filename]):
            return False
        return True

    def download(self):
        import zipfile

        if self._check_integrity():
            print('Files already downloaded and verified')
            return

        filename = self._get_target_folder()
        zip_filename = filename + '.zip'
        url = self.download_url_prefix + '/' + zip_filename
        download_url(url, self.root, zip_filename, self.zips_md5[filename])
        print('Extracting downloaded file: ' + join(self.root, zip_filename))
        with zipfile.ZipFile(join(self.root, zip_filename), 'r') as zip_file:
            zip_file.extractall(self.root)

    def _get_target_folder(self):
        return 'images_background' if self.background else 'images_evaluation'


def loadimgs(path,n = 0):
    '''
    path => Path of train directory or test directory
    '''
    X=[]
    y = []
    cat_dict = {}
    lang_dict = {}
    curr_y = n
    
    # we load every alphabet seperately so we can isolate them later
    for alphabet in os.listdir(path):
        print("loading alphabet: " + alphabet)
        lang_dict[alphabet] = [curr_y,None]
        alphabet_path = os.path.join(path,alphabet)
        
        # every letter/category has it's own column in the array, so  load seperately
        for letter in os.listdir(alphabet_path):
            cat_dict[curr_y] = (alphabet, letter)
            category_images=[]
            letter_path = os.path.join(alphabet_path, letter)
            
            # read all the images in the current category
            for filename in os.listdir(letter_path):
                image_path = os.path.join(letter_path, filename)
                image = imread(image_path)
                category_images.append(image)
                y.append(curr_y)
            try:
                X.append(np.stack(category_images))
            # edge case  - last one
            except ValueError as e:
                print(e)
                print("error - category_images:", category_images)
            curr_y += 1
            lang_dict[alphabet][1] = curr_y - 1
    y = np.vstack(y)
    X = np.stack(X)
    return X,y,lang_dict

def get_batch(batch_size,s="train"):
    """
    Create batch of n pairs, half same class, half different class
    """
    if s == 'train':
        X = Xtrain
        categories = train_classes
    else:
        X = Xval
        categories = val_classes
    n_classes, n_examples, w, h = X.shape
    
    # randomly sample several classes to use in the batch
    categories = rng.choice(n_classes,size=(batch_size,),replace=False)
    
    # initialize 2 empty arrays for the input image batch
    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]
    
    # initialize vector for the targets
    targets=np.zeros((batch_size,))
    
    # make one half of it '1's, so 2nd half of batch has same class
    targets[batch_size//2:] = 1
    for i in range(batch_size):
        category = categories[i]
        idx_1 = rng.randint(0, n_examples)
        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)
        idx_2 = rng.randint(0, n_examples)
        
        # pick images of same class for 1st half, different for 2nd
        if i >= batch_size // 2:
            category_2 = category  
        else: 
            # add a random number to the category modulo n classes to ensure 2nd image has a different category
            category_2 = (category + rng.randint(1,n_classes)) % n_classes
        
        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)
    
return pairs, targetsimport numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Reshape, ZeroPadding2D 
from keras.layers import Conv2D, UpSampling2D, MaxPooling2D
from keras.utils import np_utils
import wandb
from wandb.wandb_keras import WandbKerasCallback
import random
import cv2

# logging code
run = wandb.init()
config = run.config

# load data
(X_train, _), (X_test, _) = mnist.load_data()

img_width = X_train.shape[1]
img_height = X_train.shape[2]


X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

X_train_noise = X_train + (numpy.random.randn(28*28*60000) * 0.2).reshape(60000, 28, 28) 
X_train_noise.clip(0., 1.)

X_test_noise = X_test + (numpy.random.randn(28*28*10000) * 0.2).reshape(10000, 28, 28) 
X_test_noise.clip(0., 1.)

# create model
model=Sequential()
#model.add(Flatten(input_shape=(img_width,img_height)))
model.add(Reshape((28,28,1), input_shape=(28,28)))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(32,
    (3,3),
    activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(ZeroPadding2D((1,1), input_shape=(28, 28,1)))
model.add(Conv2D(32,
    (3,3),
    activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(UpSampling2D((2,2)))
model.add(ZeroPadding2D((1,1), input_shape=(28, 28,1)))
model.add(Conv2D(1,
    (3,3),
    activation='relu'))
model.add(UpSampling2D((2,2)))
model.add(Reshape((28,28)))
model.summary()

model.compile(loss='mse', optimizer='adam',
                metrics=['accuracy'])

# Fit the model
model.fit(X_train_noise, X_train, epochs=10, validation_data=(X_test_noise, X_test),
                    callbacks=[WandbKerasCallback()])

X_pred = model.predict([X_test_noise[:10]])
print(X_pred)
X_test_noise *= 255
cv2.imwrite('input.png', X_test_noise[0].reshape(28,28,1))
X_pred *= 255.
cv2.imwrite('output.png', X_pred[0].reshape(28,28,1))
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Perceptron
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = [x.flatten() for x in X_train]
y_train = to_categorical(y_train)

perceptron = Perceptron()

scores = cross_val_score(perceptron, X_train, y_train, cv=10)
print(scores)
print(scores.mean())
from sklearn.metrics import log_loss

truth = [0,0,0,0,1]
pred = [0.2,0.2,0.2,0.2,0.2]

print(log_loss(truth, pred))
import tensorflow as tf
import wandb

# logging code
run = wandb.init()
config = run.config

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]

num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(num_classes))
model.compile(loss='mse', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels, save_model=False)])
import tensorflow as tf
import wandb

wandb.init()

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datsets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# normalize data
X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(Flatten(input_shape=(img_width, img_height)))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'model', monitor='val_acc', verbose=1, save_best_only=True, mode='max')

# Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test),
          callbacks=[checkpoint, wandb.keras.WandbCallback])
import tensorflow as tf
import wandb

# logging code
run = wandb.init()
config = run.config

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]

num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(num_classes))
model.compile(loss='mse', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels, save_model=False)])
model.save('model.h5')
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

model = tf.keras.models.load_model("perceptron.h5")

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
num_classes = y_test.shape[1]

print(model.layers[1].get_weights())
import wandb
import tensorflow as tf

# logging code
run = wandb.init()
config = run.config

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# normalize data
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]

num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels, save_model=False)])
import tensorflow as tf
import wandb

# logging code
run = wandb.init()

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

is_five_train = y_train == 5
is_five_test = y_test == 5
labels = ["Not Five", "Is Five"]

img_width = X_train.shape[1]
img_height = X_train.shape[2]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(1))
model.compile(loss='mse', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, is_five_train, epochs=3, validation_data=(X_test, is_five_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels, save_model=False)])
model.save('perceptron.h5')
import tensorflow as tf
import wandb

# logging code
run = wandb.init()
config = run.config

# load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
img_width = X_train.shape[1]
img_height = X_train.shape[2]

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
labels = [str(i) for i in range(10)]

num_classes = y_train.shape[1]

# create model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[wandb.keras.WandbCallback(data_type="image", labels=labels, save_model=False)])
import os
import tensorflow as tf
from tensorflow.python.client import timeline
import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config
config.first_layer_convs = 32
config.first_layer_conv_width = 3
config.first_layer_conv_height = 3
config.dropout = 0.2
config.dense_layer_size = 128
config.img_width = 32
config.img_height = 32
config.channels = 3
config.epochs = 4

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()

X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# reshape input data
# X_train = X_train.reshape(
#    X_train.shape[0], config.img_width, config.img_height, 1)
# X_test = X_test.reshape(
#    X_test.shape[0], config.img_width, config.img_height, 1)

# one hot encode outputs
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
num_classes = y_test.shape[1]
labels = [str(i) for i in range(10)]

# build model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32,
                                 (config.first_layer_conv_width,
                                  config.first_layer_conv_height),
                                 input_shape=(config.img_width,
                                              config.img_height, config.channels),
                                 activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(config.dense_layer_size, activation='relu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

# optional profiling setup...
#run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
#run_metadata = tf.RunMetadata()
# options=run_options, run_metadata=run_metadata,
model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])
# log the number of total parameters
config.total_params = model.count_params()
print("Total params: ", config.total_params)

model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=config.epochs,
          callbacks=[WandbCallback(data_type="image", save_model=False),
                     tf.keras.callbacks.TensorBoard(log_dir=wandb.run.dir)])
model.save('cnn.h5')

# optional profiling setup continued
#tl = timeline.Timeline(run_metadata.step_stats)
# with open('profile.json', 'w') as f:
#    f.write(tl.generate_chrome_trace_format())

# Convert to TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_keras_model_file('cnn.h5')
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_model = converter.convert()
open("cnn.tflite", "wb").write(tflite_model)
# https://gist.github.com/tonyreina/5bbe050c2cfceae62a1dda7d9010b692
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
import tensorflow as tf
from tensorflow.python.client import timeline
import numpy as np

sess = tf.keras.backend.get_session()
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
model = InceptionV3()

random_input = np.random.random((64, 299, 299, 3))
preds = sess.run(model.output, feed_dict={model.input: random_input},
                 options=run_options, run_metadata=run_metadata)

tl = timeline.Timeline(run_metadata.step_stats)
with open('profile.json', 'w') as f:
    f.write(tl.generate_chrome_trace_format())

print("Trace saved to profile.json, open with chrome://tracing")
from tensorflow.python import pywrap_tensorflow
import h5py
import numpy as np
from keras.datasets import mnist
from keras.models import load_model
from keras.utils import np_utils
from keras.losses import categorical_crossentropy


def log_softmax(w):
    assert len(w.shape) == 1
    max_weight = np.max(w, axis=0)
    rightHandSize = np.log(np.sum(np.exp(w - max_weight), axis=0))
    return w - (max_weight + rightHandSize)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)



def load_weights_from_tensorflow(filename):
    reader = pywrap_tensorflow.NewCheckpointReader(filename)
    weights = reader.get_tensor('Variable')
    return weights

def load_biases_from_tensorflow(filename):
    reader = pywrap_tensorflow.NewCheckpointReader(filename)
    bias = reader.get_tensor('Variable_1')
    return bias

def load_weights_from_keras_perceptron(filename):
    f = h5py.File(filename)
    bias = f['model_weights']['dense_1']['dense_1']['bias:0'][()]
    weights = f['model_weights']['dense_1']['dense_1']['kernel:0'][()]
    return weights, bias


def load_weights_from_keras_two_layer(filename):
    f = h5py.File(filename)
    bias1 = (f['model_weights']['dense_2']['dense_2']["bias:0"][()])
    weights1 = (f['model_weights']['dense_2']['dense_2']["kernel:0"][()])
    bias0 = (f['model_weights']['dense_1']['dense_1']["bias:0"][()])
    weights0 = (f['model_weights']['dense_1']['dense_1']["kernel:0"][()])

    return weights0, bias0, weights1, bias1

weight0, bias0 = load_weights_from_keras_perceptron('perceptron.h5')
#weights0, bias0, weights1, bias1 = load_weights_from_keras_two_layer('two-layer.h5')

(X_train, y_train), (X_test, y_test) = mnist.load_data()

img_width=28
img_height=28

X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]

model = load_model("perceptron.h5")
print(model.predict(X_test[:1]))

x= X_test[:1].flat
output = np.zeros(10)
for i in range(10):
    out = 0
    for j in range(len(x)):
        out = out + weight0[j,i] * x[j]
    print(out)
    output[i] = out

print
softmax(output)
from tensorflow.python import pywrap_tensorflow
import h5py
import numpy as np
from keras.datasets import mnist
from keras.models import load_model
from keras.utils import np_utils
from keras.losses import categorical_crossentropy
from sklearn.metrics import log_loss


def log_softmax(w):
    assert len(w.shape) == 1
    max_weight = np.max(w, axis=0)
    rightHandSize = np.log(np.sum(np.exp(w - max_weight), axis=0))
    return w - (max_weight + rightHandSize)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

def load_weights_from_tensorflow(filename):
    reader = pywrap_tensorflow.NewCheckpointReader(filename)
    weights = reader.get_tensor('Variable')
    return weights

def load_biases_from_tensorflow(filename):
    reader = pywrap_tensorflow.NewCheckpointReader(filename)
    bias = reader.get_tensor('Variable_1')
    return bias

def load_weights_from_keras_perceptron(filename):
    f = h5py.File(filename)
    bias = f['model_weights']['dense_1']['dense_1']['bias:0'][()]
    weights = f['model_weights']['dense_1']['dense_1']['kernel:0'][()]
    return weights, bias


def load_weights_from_keras_two_layer(filename):
    f = h5py.File(filename)
    bias1 = (f['model_weights']['dense_2']['dense_2']["bias:0"][()])
    weights1 = (f['model_weights']['dense_2']['dense_2']["kernel:0"][()])
    bias0 = (f['model_weights']['dense_1']['dense_1']["bias:0"][()])
    weights0 = (f['model_weights']['dense_1']['dense_1']["kernel:0"][()])

    return weights0, bias0, weights1, bias1

weight0, bias0 = load_weights_from_keras_perceptron('perceptron.h5')
#weights0, bias0, weights1, bias1 = load_weights_from_keras_two_layer('two-layer.h5')

(X_train, y_train), (X_test, y_test) = mnist.load_data()

img_width=28
img_height=28

X_train = X_train.astype('float32')
X_train /= 255.
X_test = X_test.astype('float32')
X_test /= 255.

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]

first_digit = X_test[:1]

model = load_model("perceptron.h5")

# predicting with our model
predictions = model.predict(first_digit)
first_digit_ground_truth = y_test[:1]

loss = log_loss(first_digit_ground_truth[0], predictions[0])

# Can you calculate the predictions from weights0, bias0 and first_digit?

# Can you calculate the derivative of a weight with respect to the loss function?

# try calculating predictions and derivatives with the two-layer mlp

# try calculating predictions and derivatives with a convolutional neural net

# You can write your own convolution function  or use scipy's

# For some crazy reason, have to invert the kernel array
# return scipy.ndimage.convolve(matrix, kernel[::-1, ::-1], mode='constant' )
import numpy as np
import keras
from keras.datasets import reuters
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing.text import Tokenizer
import wandb
from wandb.keras import WandbCallback

wandb.init()
config = wandb.config

config.max_words = 1000
config.batch_size = 32
config.epochs = 5

print('Loading data...')
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=config.max_words,
                                                         test_split=0.2)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

num_classes = np.max(y_train) + 1
print(num_classes, 'classes')

# Vectorizing sequence data...
tokenizer = Tokenizer(num_words=config.max_words)
x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')
x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')

# One hot encoding
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

model = Sequential()
model.add(Dense(num_classes, input_shape=(config.max_words,), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    batch_size=config.batch_size,
                    epochs=config.epochs,
                    validation_data=(x_test, y_test), callbacks=[WandbCallback()])


from keras.models import Sequential
from keras.layers import LSTM, TimeDistributed, RepeatVector, Dense
import numpy as np
import wandb
from wandb.keras import WandbCallback

wandb.init()
config = wandb.config

class CharacterTable(object):
    """Given a set of characters:
    + Encode them to a one hot integer representation
    + Decode the one hot integer representation to their character output
    + Decode a vector of probabilities to their character output
    """
    def __init__(self, chars):
        """Initialize character table.
        # Arguments
            chars: Characters that can appear in the input.
        """
        self.chars = sorted(set(chars))
        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))
        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))

    def encode(self, C, num_rows):
        """One hot encode given string C.
        # Arguments
            num_rows: Number of rows in the returned one hot encoding. This is
                used to keep the # of rows for each data the same.
        """
        x = np.zeros((num_rows, len(self.chars)))
        for i, c in enumerate(C):
            x[i, self.char_indices[c]] = 1
        return x

    def decode(self, x, calc_argmax=True):
        if calc_argmax:
            x = x.argmax(axis=-1)
        return ''.join(self.indices_char[x] for x in x)

# Parameters for the model and dataset.
config.training_size = 50000
config.digits = 3
config.reverse = True
config.hidden_size = 128
config.batch_size = 128

# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of
# int is DIGITS.
maxlen = config.digits + 1 + config.digits

# All the numbers, plus sign and space for padding.
chars = '0123456789+ '
ctable = CharacterTable(chars)

questions = []
expected = []
seen = set()
print('Generating data...')
while len(questions) < config.training_size:
    f = lambda: int(''.join(np.random.choice(list('0123456789'))
                    for i in range(np.random.randint(1, config.digits + 1))))
    a, b = f(), f()
    # Skip any addition questions we've already seen
    # Also skip any such that x+Y == Y+x (hence the sorting).
    key = tuple(sorted((a, b)))
    if key in seen:
        continue
    seen.add(key)
    # Pad the data with spaces such that it is always MAXLEN.
    q = '{}+{}'.format(a, b)
    query = q + ' ' * (maxlen - len(q))
    ans = str(a + b)
    # Answers can be of maximum size DIGITS + 1.
    ans += ' ' * (config.digits + 1 - len(ans))
    if config.reverse:
        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the
        # space used for padding.)
        query = query[::-1]
    questions.append(query)
    expected.append(ans)
    
print('Total addition questions:', len(questions))

print('Vectorization...')
x = np.zeros((len(questions), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(questions), config.digits + 1, len(chars)), dtype=np.bool)
for i, sentence in enumerate(questions):
    x[i] = ctable.encode(sentence, maxlen)
for i, sentence in enumerate(expected):
    y[i] = ctable.encode(sentence, config.digits + 1)

# Shuffle (x, y) in unison as the later parts of x will almost all be larger
# digits.
indices = np.arange(len(y))
np.random.shuffle(indices)
x = x[indices]
y = y[indices]

# Explicitly set apart 10% for validation data that we never train over.
split_at = len(x) - len(x) // 10
(x_train, x_val) = x[:split_at], x[split_at:]
(y_train, y_val) = y[:split_at], y[split_at:]

print('Training Data:')
print(x_train.shape)
print(y_train.shape)

print('Validation Data:')
print(x_val.shape)
print(y_val.shape)



model = Sequential()
# "Encode" the input sequence using an RNN, producing an output of HIDDEN_SIZE.
# Note: In a situation where your input sequences have a variable length,
# use input_shape=(None, num_feature).
model.add(LSTM(config.hidden_size, input_shape=(maxlen, len(chars))))
# As the decoder RNN's input, repeatedly provide with the last hidden state of
# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum
# length of output, e.g., when DIGITS=3, max output is 999+999=1998.
model.add(RepeatVector(config.digits + 1))
model.add(LSTM(config.hidden_size, return_sequences=True))

# Apply a dense layer to the every temporal slice of an input. For each of step
# of the output sequence, decide which character should be chosen.
model.add(TimeDistributed(Dense(len(chars), activation='softmax')))
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()

# Train the model each generation and show predictions against the validation
# dataset.
for iteration in range(1, 200):
    print()
    print('-' * 50)
    print('Iteration', iteration)
    model.fit(x_train, y_train,
              batch_size=config.batch_size,
              epochs=1,
              validation_data=(x_val, y_val),callbacks=[WandbCallback()])
    # Select 10 samples from the validation set at random so we can visualize
    # errors.
    for i in range(10):
        ind = np.random.randint(0, len(x_val))
        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]
        preds = model.predict_classes(rowx, verbose=0)
        q = ctable.decode(rowx[0])
        correct = ctable.decode(rowy[0])
        guess = ctable.decode(preds[0], calc_argmax=False)
        print('Q', q[::-1] if config.reverse else q, end=' ')
        print('T', correct, end=' ')
        if correct == guess:
            print('', end=' ')
        else:
            print('', end=' ')
        print(guess)
# A very simple perceptron for classifying american sign language letters
import signdata
import numpy as np
from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Reshape, Add, Concatenate, Input
from keras.utils import np_utils
import wandb
from wandb.keras import WandbCallback

# logging code
run = wandb.init()
config = run.config
config.loss = "mae"
config.optimizer = "adam"
config.epochs = 10

# load data
(X_test, y_test) = signdata.load_test_data()
(X_train, y_train) = signdata.load_train_data()

img_width = X_test.shape[1]
img_height = X_test.shape[2]

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_train.shape[1]

# you may want to normalize the data here..

# normalize data
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

# create model
inp = Input(shape=(28,28))
flat_out = Flatten()(inp)
dense1_out = Dense(50, activation="softmax")(flat_out)
dense2_out = Dense(50, activation="relu")(flat_out)
dense_sum = Concatenate()([dense1_out, dense2_out])
dense_last_out = Dense(num_classes, activation="softmax")(dense_sum)
model = Model(inp, dense_last_out)


model.compile(loss=config.loss, optimizer=config.optimizer,
              metrics=['accuracy'])
model.summary()
# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[WandbCallback(data_type="image", labels=signdata.letters)])
# A very simple perceptron for classifying american sign language letters
import signdata
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Reshape
from keras.utils import np_utils
import wandb
from wandb.keras import WandbCallback

# logging code
run = wandb.init()
config = run.config
config.loss = "mae"
config.optimizer = "adam"
config.epochs = 10

# load data
(X_test, y_test) = signdata.load_test_data()
(X_train, y_train) = signdata.load_train_data() # y data is the corresponding alphabet number. There are only 25 because 2 alphabets are removed as they involve hand movement in the sign language

img_width = X_test.shape[1]
img_height = X_test.shape[2]

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_train.shape[1]

# create model
model = Sequential()
model.add(Flatten())
model.add(Dense(num_classes, activation="softmax"))
model.compile(loss=config.loss, optimizer=config.optimizer,
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),
          callbacks=[WandbCallback(data_type="image", labels=signdata.letters)])

# The default model in this case is performing very badly. The accuracy is pretty bad.
# Normalizing the data improves the accuracy. One way to do it for this data is to divide the data by 255 Since the input is between 0 and 255
# # Code to load the sign data

import os.path
import numpy as np
import pandas as pd
import wandb
import subprocess

if not os.path.isfile('sign-language/sign_mnist_train.csv'):
    print("Downloading signlanguage dataset...")
    subprocess.check_output("curl https://storage.googleapis.com/wandb-production.appspot.com/mlclass/sign-language-data.tar.gz | tar xvz", shell=True)

letters = "abcdefghijklmnopqrstuvwxyz"

def load_train_data():
    df=pd.read_csv('sign-language/sign_mnist_train.csv')
    X = df.values[:,1:].reshape(-1,28,28)
    y = df.values[:,0]
    return X, y

def load_test_data():
    df=pd.read_csv('sign-language/sign_mnist_test.csv')
    X = df.values[:,1:].reshape(-1,28,28)
    y = df.values[:,0]
    return X, y

X,y = load_train_data()
test_X, test_y = load_test_data()

import numpy as np
import cv2
import argparse
import os
import keras
import signdata

def preprocess(gray_hand):
    # Be sure to do the same preprocessing here that you did in your training!
    gray_hand = gray_hand.astype('float32')
    gray_hand /= 255.
    return gray_hand


def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,
                                                font_scale=2, thickness=2):
    x, y = coordinates[:2]
    cv2.putText(image_array, text, (x + x_offset, y + y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                font_scale, color, thickness, cv2.LINE_AA)

def load_detection_model(model_path):
    detection_model = cv2.CascadeClassifier(model_path)
    return detection_model

def detect_hands(detection_model, gray_image_array):
    return detection_model.detectMultiScale(gray_image_array, 1.3, 5)

def draw_bounding_box(face_coordinates, image_array, color):
    x, y, w, h = face_coordinates
    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)


mode = "fixed"
fist_model = load_detection_model("aGest.xml")
palm_model = load_detection_model("palm.xml")

hand_target_size = (28, 28)

parser = argparse.ArgumentParser()
parser.add_argument("model", help="path to model file")
args = parser.parse_args()

if not os.path.exists(args.model):
    parser.error("The file %s does not exist!" % args.model)

model = keras.models.load_model(args.model)

cap = cv2.VideoCapture(0)

while(True):
    # Capture frame-by-frame
    ret, frame = cap.read()

    # Our operations on the frame come here
    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray_image = cv2.flip( gray_image, 1 )
    hands = []
    if mode == "detect":
        fists = detect_hands(fist_model, gray_image)
        palms = detect_hands(palm_model, gray_image)
    
        for bbox in fists:
            hands.append(bbox)
        for bbox in palms:
            hands.append(bbox)
    else:
        hands = [[100,100,300,300]]
    
    for bbox in hands:
        x, y, width, height = bbox
        x1 = x
        y1 = y
        x2 = x + width
        y2 = y + height

        gray_hand = gray_image[x1:x2, y1:y2]
        try:
            gray_hand = cv2.resize(gray_hand, (hand_target_size))
            gray_hand = preprocess(gray_hand)
        except:
            print("There was an exception!")
            continue

        print("Max: ", np.max(gray_hand), "Min: ", np.min(gray_hand))

        gray_hand = np.expand_dims(gray_hand, 0)

        color = (255,255,255)
        draw_bounding_box(bbox, gray_image, color)
        preds = model.predict(gray_hand)
        p = np.argmax(preds)
        prob = np.max(preds)
        text = signdata.letters[p] + " " + str(prob)
        text_box = (bbox[0], bbox[1]+bbox[3]+60)
        draw_text(text_box,gray_image,text,color)
        
    # Display the resulting frame
    cv2.imshow('frame',gray_image)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# When everything done, release the capture
cap.release()
cv2.destroyAllWindows()
# much of the code borrowed and modified from https://github.com/kylemcdonald/SmileCNN
# need to do wget https://github.com/hromi/SMILEsmileD/archive/master.zip

import numpy as np
from skimage.measure import block_reduce
from skimage.io import imread
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Flatten, Reshape
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
from glob import glob

import wandb
from wandb.wandb_keras import WandbKerasCallback

run = wandb.init()
config = run.config

negative_paths = glob('SMILEsmileD-master/SMILEs/negatives/negatives7/*.jpg')
positive_paths = glob('SMILEsmileD-master/SMILEs/positives/positives7/*.jpg')
examples = [(path, 0) for path in negative_paths] + [(path, 1) for path in positive_paths]

def examples_to_dataset(examples, block_size=2):
    X = []
    y = []
    for path, label in examples:
        img = imread(path, as_grey=True)
        img = block_reduce(img, block_size=(block_size, block_size), func=np.mean)
        X.append(img)
        y.append(label)
    return np.asarray(X), np.asarray(y)

X, y = examples_to_dataset(examples)

X = X.astype(np.float32) / 255.
y = y.astype(np.int32)

# convert classes to vector
nb_classes = 2
y = np_utils.to_categorical(y, nb_classes).astype(np.float32)

# shuffle all the data
indices = np.arange(len(X))
np.random.shuffle(indices)

train_cutoff = int(len(X) * 0.9)
print(train_cutoff)

X_train = X[indices[:train_cutoff]]
y_train = y[indices[:train_cutoff]]

X_val = X[indices[train_cutoff:]]
y_val = y[indices[train_cutoff:]]



# prepare weighting for classes since they're unbalanced
class_totals = y.sum(axis=0)
class_weight = class_totals.max() / class_totals

img_rows, img_cols = X.shape[1:]
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)
X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(img_rows,img_cols,1)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes, activation='softmax'))
model.summary()
#from pdb import set_trace;set_trace()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

datagen = ImageDataGenerator(
    #featurewise_center=True,
    #samplewise_center=True,
    rotation_range=2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
print(X_train.shape)

model.fit_generator(datagen.flow(X_train, y_train, batch_size=128), class_weight=class_weight,
                    nb_epoch=1, verbose=1, steps_per_epoch=100,
                    callbacks=[WandbKerasCallback()],
                    validation_data=(X_val, y_val))
model.save("smile.h5")
import flask
import keras
import numpy as np
import os
from keras.models import load_model
from PIL import Image
from flask import Flask, request
from jinja2 import Template

app = Flask(__name__)

model = load_model('smile.h5')
model._make_predict_function()


def predict_image(image):
    image = image.convert(mode="L")
    image = image.resize((32, 32))
    im = np.asarray(image)
    im = im.reshape(1, 32, 32, 1)
    pred = model.predict(im)
    return pred


@app.route("/predict", methods=["POST"])
def predict():
    f = request.files['file']
    image = Image.open(f.stream)
    pred = predict_image(image)
    template = Template("""
        <html>
            <body>
                <p>Probability of Smiling: {{smile_prob}}</p>
                <p>Probability of Not Smiling: {{no_smile_prob}}</p>
            </body>
        </html>
    """)

    return template.render(smile_prob=pred[0], no_smile_prob=pred[1])


@app.route("/")
def index():
    html = """
    <html>
        <body>
            <form action="predict" method="POST" enctype="multipart/form-data">
                <input type="file" name="file" accept="image/*;capture=camera">
                <input type="submit"/>
            </form>
        </body>
    </html>
    """
    return(html)


if __name__ == '__main__' and not os.getenv("FLASK_DEBUG"):
    app.run(port=8080)
import flask
import keras
import numpy as np
import os
from keras.models import load_model
from PIL import Image
from flask import Flask, request
from jinja2 import Template

app = Flask(__name__)

model = load_model('smile.h5')
model._make_predict_function()


def predict_image(image):
    image = image.convert(mode="L")
    image = image.resize((64, 64))
    im = np.asarray(image)
    im_rescale = im.reshape(1, 64, 64, 1)
    pred = model.predict(im_rescale)
    return pred[0]


@app.route("/predict", methods=["POST"])
def predict():
    f = request.files['file']
    image = Image.open(f.stream)
    pred = predict_image(image)
    template = Template("""
        <html>
            <body>
                <p>Probability of Smiling: {{smile_prob}}</p>
                <p>Probability of Not Smiling: {{no_smile_prob}}</p>
            </body>
        </html>
    """)

    return template.render(smile_prob=pred[0], no_smile_prob=pred[1])


@app.route("/")
def index():
    html = """
    <html>
        <body>
            <form action="predict" method="POST" enctype="multipart/form-data">
                <input type="file" name="file" accept="image/*;capture=camera">
                <input type="submit"/>
            </form>
        </body>
    </html>
    """
    return(html)


if __name__ == '__main__' and not os.getenv("FLASK_DEBUG"):
    app.run(port=8080)
import flask
import keras
import numpy as np
import os
from keras.models import load_model
from PIL import Image, ExifTags
from flask import Flask, request
from jinja2 import Template
import base64

app = Flask(__name__)

model = load_model('smile.h5')
model._make_predict_function()


def maybe_rotate(image):
    try:
        for orientation in ExifTags.TAGS.keys():
            if ExifTags.TAGS[orientation] == 'Orientation':
                break
        exif = dict(image._getexif().items())
        if exif[orientation] == 3:
            image = image.rotate(180, expand=True)
        elif exif[orientation] == 6:
            image = image.rotate(270, expand=True)
        elif exif[orientation] == 8:
            image = image.rotate(90, expand=True)
        return image
    except (AttributeError, KeyError, IndexError):
        # cases: image don't have getexif
        return image


def predict_image(image):
    image = maybe_rotate(image)
    image = image.convert(mode="L")
    image = image.resize((64, 64))
    im = np.asarray(image)
    im = im.reshape(1, 64, 64, 1)

    im_rescale = im / 255.0
    pred = model.predict(im_rescale)
    return pred[0], image


@app.route("/predict", methods=["POST"])
def predict():
    f = request.files['file']
    image = Image.open(f.stream)
    pred, image = predict_image(image)
    image.save("/tmp/thumb_file.jpg")
    with open("/tmp/thumb_file.jpg", "rb") as img:
        thumb_string = base64.b64encode(img.read())
        base64out = "data:image/jpeg;base64," + str(thumb_string)[2:-1]
    template = Template("""
        <html>
            <body>
                <img src="{{face}}" style="width:200px" />
                <p>Probability of Smiling: {{smile_prob}}</p>
                <p>Probability of Not Smiling: {{no_smile_prob}}</p>
            </body>
        </html>
    """)

    return template.render(smile_prob="%.2f" % pred[1], no_smile_prob="%.2f" % pred[0], face=base64out)


@app.route("/")
def index():
    html = """
    <html>
        <body>
            <form action="predict" method="POST" enctype="multipart/form-data">
                <input type="file" name="file" accept="image/*;capture=camera">
                <input type="submit"/>
            </form>
        </body>
    </html>
    """
    return(html)


if __name__ == '__main__' and not os.getenv("FLASK_DEBUG"):
    app.run(port=8080)
import flask
import keras
import numpy as np
import os
from keras.models import load_model
from PIL import Image, ExifTags
from flask import Flask, request
from jinja2 import Template
import base64
import cv2

app = Flask(__name__)

model = load_model('smile.h5')
model._make_predict_function()

detection_model = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')


def maybe_rotate(image):
    try:
        for orientation in ExifTags.TAGS.keys():
            if ExifTags.TAGS[orientation] == 'Orientation':
                break
        exif = dict(image._getexif().items())
        if exif[orientation] == 3:
            image = image.rotate(180, expand=True)
        elif exif[orientation] == 6:
            image = image.rotate(270, expand=True)
        elif exif[orientation] == 8:
            image = image.rotate(90, expand=True)
        return image
    except (AttributeError, KeyError, IndexError):
        # cases: image don't have getexif
        return image


def detect_face(image, offsets=(0, 0)):
    faces = detection_model.detectMultiScale(image, 1.3, 5)
    if len(faces) > 0:
        x, y, width, height = faces[0]
        x_off, y_off = offsets
        x1, x2, y1, y2 = (x - x_off, x + width + x_off,
                          y - y_off, y + height + y_off)
        return cv2.resize(image[y1:y2, x1:x2], (64, 64))
    else:
        print("No faces found... using entire image")
        return cv2.resize(image, (64, 64))


def predict_image(image):
    image = maybe_rotate(image)
    image = image.convert(mode="L")
    im = np.asarray(image)
    face = detect_face(im)
    im_reshape = face.reshape(1, 64, 64, 1)
    im_rescale = im_reshape / 255.0
    pred = model.predict(im_rescale)
    return pred[0], face


@app.route("/predict", methods=["POST"])
def predict():
    f = request.files['file']
    image = Image.open(f.stream)
    pred, image = predict_image(image)
    cv2.imwrite("/tmp/thumb_file.jpg", image)
    with open("/tmp/thumb_file.jpg", "rb") as img:
        thumb_string = base64.b64encode(img.read())
        base64out = "data:image/jpeg;base64," + str(thumb_string)[2:-1]
    template = Template("""
        <html>
            <body>
                <img src="{{face}}" style="width:200px" />
                <p>Probability of Smiling: {{smile_prob}}</p>
                <p>Probability of Not Smiling: {{no_smile_prob}}</p>
            </body>
        </html>
    """)

    return template.render(smile_prob="%.4f" % pred[1], no_smile_prob="%.4f" % pred[0], face=base64out)


@app.route("/")
def index():
    html = """
    <html>
        <body>
            <form action="predict" method="POST" enctype="multipart/form-data">
                <input type="file" name="file" accept="image/*;capture=camera">
                <input type="submit"/>
            </form>
        </body>
    </html>
    """
    return(html)


if __name__ == '__main__' and not os.getenv("FLASK_DEBUG"):
    app.run(port=8080)
# need to download data:
# Linux/Windows
#              wget https://github.com/hromi/SMILEsmileD/archive/master.zip
# Mac
#              curl -O -L https://github.com/hromi/SMILEsmileD/archive/master.zip
# then run unzip master.zip
#
# If you get an error about skimage, you also may need to run pip install scikit-image
#

import numpy as np
import os
import subprocess
from skimage.measure import block_reduce
from skimage.io import imread
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Flatten, Reshape
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils
from glob import glob

import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config

if not os.path.exists("SMILEsmileD-master"):
    print("Downloading dataset...")
    subprocess.check_output(
        "wget https://github.com/hromi/SMILEsmileD/archive/master.zip; unzip master.zip; rm master.zip", shell=True)

negative_paths = glob('SMILEsmileD-master/SMILEs/negatives/negatives7/*.jpg')
positive_paths = glob('SMILEsmileD-master/SMILEs/positives/positives7/*.jpg')
examples = [(path, 0) for path in negative_paths] + [(path, 1)
                                                     for path in positive_paths]


def examples_to_dataset(examples, block_size=2):
    X = []  # pixels
    y = []  # labels
    for path, label in examples:
        # read the images
        img = imread(path, as_gray=True)

        # scale down the images
        # img = block_reduce(img, block_size=(block_size, block_size), func=np.mean)

        X.append(img)
        y.append(label)
    return np.asarray(X), np.asarray(y)


X, y = examples_to_dataset(examples)

X = X.astype(np.float32) / 255.
y = y.astype(np.int32)

# convert classes to vector
nb_classes = 2
y = np_utils.to_categorical(y, nb_classes).astype(np.float32)

# shuffle all the data
indices = np.arange(len(X))
np.random.shuffle(indices)
X = X[indices]
y = y[indices]

# prepare weighting for classes since they're unbalanced
class_totals = y.sum(axis=0)
class_weight = class_totals.max() / class_totals

img_rows, img_cols = X.shape[1:]

# add additional dimension
X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu',
                 input_shape=(img_rows, img_cols, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

model.fit(X, y, batch_size=config.batch_size, class_weight=class_weight,
          epochs=config.epochs, verbose=1,
          validation_split=0.1, callbacks=[WandbCallback(data_type="image")])

model.save("smile.h5")
from subprocess import call
import os
from urllib.request import urlretrieve
from keras.preprocessing.image import load_img, img_to_array
import numpy as np

dataset_url = "http://aisdatasets.informatik.uni-freiburg.de/" \
              "freiburg_groceries_dataset/freiburg_groceries_dataset.tar.gz"

def download():
    print("Downloading dataset.")
    urlretrieve(dataset_url, "freiburg_groceries_dataset.tar.gz")
    print("Extracting dataset.")
    call(["tar", "-xf", "freiburg_groceries_dataset.tar.gz", "-C", "."])
    os.remove("freiburg_groceries_dataset.tar.gz")
    print("Done.")

    
def load_data():
    if (not os.path.exists("images")):
        download()
        
    x_train = []
    y_train = []
    x_test = []
    y_test = []
    class_names = []
    category_num = 0
    
    for category in sorted(os.listdir("images")):
        class_names.append(category)
        count = 0
        for img in sorted(os.listdir(os.path.join("images", category))):
            if (not img.endswith(".png")):
                continue
                
            x = load_img(os.path.join("images", category, img),target_size=(224, 224))
            if count < 10:
                x_test.append(img_to_array(x))
                y_test.append(category_num)
            else:
                x_train.append(img_to_array(x))
                y_train.append(category_num)
            count += 1
        category_num += 1
    return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test)), class_names
    

# modified from https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html

# to get the data for this (linux, windows)
# wget https://s3.amazonaws.com/ml-lukas/dogcat.zip
# (for smaller file) wget https://s3.amazonaws.com/ml-lukas/dogcat-small.zip
#
# or mac
# curl -O https://s3.amazonaws.com/ml-lukas/dogcat.zip


# unzip dogcat.zip
# unzip dogcat-small.zip

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K

import wandb
from wandb.wandb_keras import WandbKerasCallback

run = wandb.init()
config = run.config

# dimensions of our images.
img_width, img_height = 150, 150

train_data_dir = 'dogcat-data/train'
validation_data_dir = 'dogcat-data/validation'
nb_train_samples = 2000
nb_validation_samples = 2000
epochs = 50
batch_size = 16

input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.summary()
#exit()
model.compile(loss='binary_crossentropy', optimizer='sgd',
              metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    #shear_range=0.2,
    #zoom_range=0.2,
    #horizontal_flip=True
    )

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    callbacks=[WandbKerasCallback()],
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size)

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.layers import Dropout, Flatten, Dense
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from dogcat_data import generators, get_nb_files
import os
import sys
import wandb
from wandb.keras import WandbCallback

wandb.init()
config = wandb.config

# dimensions of our images.
config.img_width = 224
config.img_height = 224
config.epochs = 50
config.batch_size = 40

top_model_weights_path = 'bottleneck.h5'
train_dir = 'dogcat-data/train'
validation_dir = 'dogcat-data/validation'
nb_train_samples = 1000
nb_validation_samples = 1000


def save_bottlebeck_features():
    if os.path.exists('bottleneck_features_train.npy') and (len(sys.argv) == 1 or sys.argv[1] != "--force"):
        print("Using saved features, pass --force to save new features")
        return
    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
    train_generator = datagen.flow_from_directory(
        train_dir,
        target_size=(config.img_width, config.img_height),
        batch_size=config.batch_size,
        class_mode="binary")

    val_generator = datagen.flow_from_directory(
        validation_dir,
        target_size=(config.img_width, config.img_height),
        batch_size=config.batch_size,
        class_mode="binary")

    # build the VGG16 network
    model = VGG16(include_top=False, weights='imagenet')

    print("Predicting bottleneck training features")
    training_labels = []
    training_features = []
    for batch in range(5):  # nb_train_samples // config.batch_size):
        data, labels = next(train_generator)
        training_labels.append(labels)
        training_features.append(model.predict(data))
    training_labels = np.concatenate(training_labels)
    training_features = np.concatenate(training_features)
    np.savez(open('bottleneck_features_train.npy', 'wb'),
             features=training_features, labels=training_labels)

    print("Predicting bottleneck validation features")
    validation_labels = []
    validation_features = []
    validation_data = []
    for batch in range(nb_validation_samples // config.batch_size):
        data, labels = next(val_generator)
        validation_features.append(model.predict(data))
        validation_labels.append(labels)
        validation_data.append(data)
    validation_labels = np.concatenate(validation_labels)
    validation_features = np.concatenate(validation_features)
    validation_data = np.concatenate(validation_data)
    np.savez(open('bottleneck_features_validation.npy', 'wb'),
             features=validation_features, labels=validation_labels, data=validation_data)


def train_top_model():
    train = np.load(open('bottleneck_features_train.npy', 'rb'))
    X_train, y_train = (train['features'], train['labels'])
    test = np.load(open('bottleneck_features_validation.npy', 'rb'))
    X_test, y_test, val_data = (test['features'], test['labels'], test['data'])

    model = Sequential()
    model.add(Flatten(input_shape=X_train[0].shape))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy', metrics=['accuracy'])

    class Images(Callback):
        def on_epoch_end(self, epoch, logs):
            base_model = VGG16(include_top=False, weights='imagenet')
            indices = np.random.randint(val_data.shape[0], size=36)
            test_data = val_data[indices]
            features = base_model.predict(
                np.array([preprocess_input(data) for data in test_data]))
            pred_data = model.predict(features)
            wandb.log({
                "examples": [
                      wandb.Image(
                          test_data[i], caption="cat" if pred_data[i] < 0.5 else "dog")
                      for i, data in enumerate(test_data)]
            }, commit=False)

    model.fit(X_train, y_train,
              epochs=config.epochs,
              batch_size=config.batch_size,
              validation_data=(X_test, y_test),
              callbacks=[Images(), WandbCallback(save_model=False)])
    model.save_weights(top_model_weights_path)


save_bottlebeck_features()
train_top_model()
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense
import wandb
from wandb.keras import WandbCallback

datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

#img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image
#x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)
#x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)

# the .flow() command below generates batches of randomly transformed images
# and saves the results to the `preview/` directory
i = 0
for batch in datagen.flow(x, batch_size=1,
                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):
    i += 1
    if i > 20:
        break  # otherwise the generator would loop indefinitely

run = wandb.init()
config = run.config

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

batch_size = 16

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# this is a generator that will read pictures found in
# subfolers of 'data/train', and indefinitely generate
# batches of augmented image data
train_generator = train_datagen.flow_from_directory(
        'data/train',  # this is the target directory
        target_size=(150, 150),  # all images will be resized to 150x150
        batch_size=batch_size,
        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels

# this is a similar generator, for validation data
validation_generator = test_datagen.flow_from_directory(
        'data/validation',
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary')

model.fit_generator(
        train_generator,
        steps_per_epoch=2000 // batch_size,
        epochs=50,
        validation_data=validation_generator,
        callbacks=[WandbKerasCallback()],
        validation_steps=800 // batch_size)


model.save_weights('first_try.h5')  # always save your weights after training or during training
import os
import sys
import glob
import argparse
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import SGD
from dogcat_data import generators, get_nb_files

import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config
config = run.config
config.img_width = 299
config.img_height = 299
config.epochs = 5
config.fc_size = 1024
config.batch_size = 32

NB_IV3_LAYERS_TO_FREEZE = 172


def add_new_last_layer(base_model, nb_classes):
    """Add last layer to the convnet
    Args:
        base_model: keras model excluding top
        nb_classes: # of classes
    Returns:
        new keras model with last layer
      """
    x = base_model.output
    x = Dense(config.fc_size, activation='relu')(
        x)  # new FC layer, random init
    predictions = Dense(nb_classes, activation='softmax')(
        x)  # new softmax layer
    model = Model(inputs=base_model.input, outputs=predictions)
    return model


def setup_to_finetune(model):
    """Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.
        note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch
    Args:
        model: keras model
    """
    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:
        layer.trainable = False
    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:
        layer.trainable = True
    model.compile(optimizer=SGD(lr=0.001, momentum=0.9),
                  loss='categorical_crossentropy', metrics=['accuracy'])


train_dir = "dogcat-data/train"
val_dir = "dogcat-data-small/validation"

nb_train_samples = get_nb_files(train_dir)
nb_classes = len(glob.glob(train_dir + "/*"))
nb_val_samples = get_nb_files(val_dir)

# data prep
train_generator, validation_generator = generators(
    preprocess_input, config.img_width, config.img_height, config.batch_size)

# setup model
# include_top=False excludes final FC layer
base_model = InceptionV3(weights='imagenet', include_top=False, pooling="avg")
model = add_new_last_layer(base_model, nb_classes)
model._is_graph_network = False

# fine-tuning
setup_to_finetune(model)

model.fit_generator(
    train_generator,
    epochs=config.epochs,
    workers=2,
    steps_per_epoch=nb_train_samples * 2 / config.batch_size,
    validation_data=validation_generator,
    validation_steps=nb_train_samples / config.batch_size,
    callbacks=[WandbCallback(data_type="image", generator=validation_generator, labels=[
                             'cat', 'dog'], save_model=False)],
    class_weight='auto')

# model.save('transfered.h5')
import os
import sys
import glob
import argparse
from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import SGD, Adam
from dogcat_data import generators, get_nb_files

import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config
#fixed size for DenseNet121
config.img_width = 224
config.img_height = 224
config.epochs = 10
config.batch_size = 128

def setup_to_transfer_learn(model, base_model):
    """Freeze all layers and compile the model"""
    for layer in base_model.layers:
        layer.trainable = False
    optimizer = Adam(lr=0.0001,
                     beta_1=0.9,
                     beta_2=0.999,
                     epsilon=None,
                     decay=0.0)
    sgd = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

def add_new_last_layer(base_model, nb_classes, activation='softmax'):
    """Add last layer to the convnet
    Args:
    base_model: keras model excluding top
    nb_classes: # of classes
    Returns:
    new keras model with last layer
    """
    predictions = Dense(nb_classes, activation=activation)(base_model.output)
    return Model(inputs=base_model.input, outputs=predictions)

train_dir = "dogcat-data/train"
val_dir = "dogcat-data/validation"
nb_train_samples = get_nb_files(train_dir)
nb_classes = len(glob.glob(train_dir + "/*"))
nb_val_samples = get_nb_files(val_dir)

train_generator, validation_generator = generators(preprocess_input, config.img_width, config.img_height, config.batch_size)

# setup model
base_model = DenseNet121(input_shape=(config.img_width, config.img_height, 3),
                         weights='imagenet',
                         include_top=False,
                         pooling='avg')

model = add_new_last_layer(base_model, nb_classes)

# transfer learning
setup_to_transfer_learn(model, base_model)

history_tl = model.fit_generator(
    train_generator,
    workers=2,
    epochs=config.epochs,
    steps_per_epoch=nb_train_samples * 2 / config.batch_size,
    validation_data=validation_generator,
    validation_steps=nb_train_samples / config.batch_size,
    callbacks=[WandbCallback(data_type="image", generator=validation_generator, labels=['cat', 'dog'], save_model=False)],
    class_weight='auto')

model.save("transfered.h5")
# modified from https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dropout, Flatten, Dense, Input
import glob
import wandb
from wandb.keras import WandbCallback
from dogcat_data import generators, get_nb_files

run = wandb.init()
config = run.config
config.img_width = 224 
config.img_height = 224
config.epochs = 50
config.batch_size = 32

inp = Input(shape=(224, 224, 3), name='input_image')

main_model = ResNet50(include_top=False, weights="imagenet")
for layer in main_model.layers:
    layer.trainable=False

main_model = main_model(inp)
main_out = Flatten()(main_model)
main_out = Dense(512, activation='relu', name='fcc_0')(main_out)
main_out = Dense(1, activation='sigmoid', name='class_id')(main_out)

model = Model(inputs=inp, outputs=main_out)
model._is_graph_network = False

# compile the model with a SGD/momentum optimizer
# and a very slow learning rate.
model.compile(loss='binary_crossentropy',
              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
              metrics=['accuracy'])

train_dir = "dogcat-data/train"
val_dir = "dogcat-data/validation"
nb_train_samples = get_nb_files(train_dir)
nb_classes = len(glob.glob(train_dir + "/*"))
nb_val_samples = get_nb_files(val_dir)

train_generator, validation_generator = generators(preprocess_input, config.img_width, config.img_height, config.batch_size, binary=True)

# fine-tune the model
model.fit_generator(
    train_generator,
    epochs=config.epochs,
    validation_data=validation_generator,
    callbacks=[WandbCallback(data_type="image", generator=validation_generator, labels=['cat', 'dog'],save_model=False)],
    workers=2,
    steps_per_epoch=nb_train_samples * 2 / config.batch_size,
    validation_steps=nb_train_samples / config.batch_size,
)
model.save('transfered.h5')
import os
import sys
import glob
import argparse
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import SGD
from dogcat_data import generators, get_nb_files

import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config
config.img_width = 299
config.img_height = 299
config.epochs = 5
config.fc_size = 1024
config.batch_size = 128


def setup_to_transfer_learn(model, base_model):
    """Freeze all layers and compile the model"""
    for layer in base_model.layers:
        layer.trainable = False
    model.compile(optimizer=SGD(lr=0.001, momentum=0.9),
                  loss='categorical_crossentropy', metrics=['accuracy'])


def add_new_last_layer(base_model, nb_classes):
    """Add last layer to the convnet
      Args:
        base_model: keras model excluding top
        nb_classes: # of classes
      Returns:
        new keras model with last layer
    """
    x = base_model.output
    x = Dense(config.fc_size, activation='relu')(
        x)  # new FC layer, random init
    predictions = Dense(nb_classes, activation='softmax')(
        x)  # new softmax layer
    model = Model(inputs=base_model.input, outputs=predictions)
    return model


train_dir = "dogcat-data/train"
val_dir = "dogcat-data/validation"

nb_train_samples = get_nb_files(train_dir)
nb_classes = len(glob.glob(train_dir + "/*"))
nb_val_samples = get_nb_files(val_dir)

# data prep
train_generator, validation_generator = generators(
    preprocess_input, config.img_width, config.img_height, config.batch_size)

# setup model
base_model = InceptionV3(weights='imagenet', include_top=False, pooling="avg")
model = add_new_last_layer(base_model, nb_classes)
model._is_graph_network = False

# transfer learning
setup_to_transfer_learn(model, base_model)

model.fit_generator(
    train_generator,
    epochs=config.epochs,
    workers=2,
    steps_per_epoch=nb_train_samples * 2 / config.batch_size,
    validation_data=validation_generator,
    validation_steps=nb_train_samples / config.batch_size,
    callbacks=[WandbCallback(data_type="image", generator=validation_generator, labels=[
                             'cat', 'dog'], save_model=False)],
    class_weight='auto')

model.save('transfered.h5')
import os
import glob
import subprocess
from tensorflow.keras.preprocessing.image import ImageDataGenerator

if not os.path.exists("dogcat-data"):
    print("Downloading dog/cat dataset...")
    subprocess.check_output("curl https://storage.googleapis.com/wandb-production.appspot.com/qualcomm/dogcat-data.tgz | tar xvz", shell=True)
    subprocess.check_output("rm dogcat-data/train/dog/._dog* dogcat-data/train/cat/._cat* dogcat-data/validation/cat/._cat* dogcat-data/validation/dog/._dog*", shell=True)

def get_nb_files(directory):
    """Get number of files by searching directory recursively"""
    if not os.path.exists(directory):
        return 0
    cnt = 0
    for r, dirs, files in os.walk(directory):
        for dr in dirs:
            cnt += len(glob.glob(os.path.join(r, dr + "/*")))
    return cnt

# data prep
def generators(preprocessing_function, img_width, img_height, batch_size=32, binary=False, shuffle=True,
               train_dir="dogcat-data/train", val_dir="dogcat-data/validation"):
    train_datagen = ImageDataGenerator(
        preprocessing_function=preprocessing_function,
        rotation_range=30,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True
    )

    test_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode="binary" if binary else "categorical",
        shuffle=shuffle
    )
    validation_generator = test_datagen.flow_from_directory(
        val_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode="binary" if binary else "categorical",
        shuffle=shuffle
    )
    return train_generator, validation_generatorfrom subprocess import call
import os
from urllib.request import urlretrieve
from keras.preprocessing.image import load_img, img_to_array
import numpy as np

dataset_url = "http://aisdatasets.informatik.uni-freiburg.de/" \
              "freiburg_groceries_dataset/freiburg_groceries_dataset.tar.gz"

def download():
    print("Downloading dataset.")
    urlretrieve(dataset_url, "freiburg_groceries_dataset.tar.gz")
    print("Extracting dataset.")
    call(["tar", "-xf", "freiburg_groceries_dataset.tar.gz", "-C", "../"])
    os.remove("freiburg_groceries_dataset.tar.gz")
    print("Done.")

    
def load_data():
    if (not os.path.exists("images")):
        download()
        
    x_train = []
    y_train = []
    x_test = []
    y_test = []
    class_names = []
    category_num = 0
    
    for category in sorted(os.listdir("images")):
        class_names.append(category)
        count = 0
        for img in sorted(os.listdir(os.path.join("images", category))):
            if (not img.endswith(".png")):
                continue
                
            x = load_img(os.path.join("images", category, img),target_size=(224, 224))
            if count < 10:
                x_test.append(img_to_array(x))
                y_test.append(category_num)
            else:
                x_train.append(img_to_array(x))
                y_train.append(category_num)
            count += 1
        category_num += 1
    return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test)), class_names
    
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions
import numpy as np

model = InceptionV3(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(299, 299))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
model.summary()
preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])
# model.save('image.h5')
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions
import numpy as np
import time

model = MobileNetV2(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
model.summary()
t = time.time()
preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])
print('Took: %.2f seconds' % (time.time() - t))
# model.save('image.h5')
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

model = ResNet50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
model.summary()
preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])
from tensorflow.keras.applications.resnext import ResNext50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnext import preprocess_input, decode_predictions
import numpy as np

model = ResNext50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
model.summary()
preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])
# pip install opencv-python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers.core import Flatten, Dense, Dropout
from tensorflow.keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import backend as K
import cv2
import numpy as np
import os

# get weights from https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc
K.set_image_dim_ordering('th')


def VGG_16(weights_path=None):
    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(3, 224, 224)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(Flatten())
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1000, activation='softmax'))

    if weights_path:
        model.load_weights(weights_path)

    return model


if __name__ == "__main__":
    model = VGG_16()
    model.summary()
    # Test pretrained model if weights exist
    if os.path.exists("vgg16_weights.h5"):
        im = cv2.resize(cv2.imread('elephant.jpg'),
                        (224, 224)).astype(np.float32)
        im[:, :, 0] -= 103.939
        im[:, :, 1] -= 116.779
        im[:, :, 2] -= 123.68
        im = im.transpose((2, 0, 1))
        im = np.expand_dims(im, axis=0)
        model = VGG_16('vgg16_weights.h5')
        sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
        model.compile(optimizer=sgd, loss='categorical_crossentropy')
        out = model.predict(im)
        print(np.argmax(out))
import pandas as pd
import numpy as np

# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# The rows of  the "emotion" column have one of three strings:
# 'Positive emotion'
# 'Negative emotion'
# 'No emotion toward brand or product'

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with an svm:
from sklearn.svm import SVC


clf = SVC()
clf.fit(counts, target)

#Try the classifier
print(clf.predict(count_vect.transform(['i do not love my iphone'])))

# See what the classifier predicts for some new tweets:
#for tweet in ('I love my iphone!!!', 'iphone costs too much!!!', 'the iphone is not good', 'I like turtles'):
#  print('Tweet: ' + tweet)
#  print('Prediction: ' + str(clf.predict(count_vect.transform([tweet]))))
#  print('\n')
import pandas as pd
import numpy as np

# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with a Naive Bayes classifier:
from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(counts, target)

#Try the classifier
print(nb.predict(count_vect.transform(['i hate my iphone'])))
import pandas as pd
import numpy as np

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from embedding import MeanEmbeddingVectorizer
from tokenizer import Tokenizer
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.ensemble import ExtraTreesClassifier


from wandblog import log
import wandb
run = wandb.init(job_type='eval')
config = run.config

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

w2v = {}
with open("glove/glove.6B.50d.txt", "r") as lines:
    for line in lines:
        word, numbers = line.split(" ", 1)
        number_array = np.array(numbers.split()).astype(np.float)
        w2v[word] = number_array


text_clf = Pipeline([('token', Tokenizer()),
                     ('vect', MeanEmbeddingVectorizer(w2v)),
                     ("extra trees", ExtraTreesClassifier(n_estimators=200)),])

text_clf.fit(fixed_text, fixed_target)

scores = cross_val_score(text_clf, fixed_text, fixed_target)
print(scores)
print(scores.mean())

predictions = cross_val_predict(text_clf, fixed_text, fixed_target)
log(run, fixed_text, fixed_target, predictions)
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

from wandblog import log
import wandb
run = wandb.init()
config = run.config

wnl = WordNetLemmatizer()

def lemma_tokenizer():
    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]

vect = CountVectorizer(tokenizer=LemmaTokenizer())

import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

count_vect = CountVectorizer(tokenizer=tokenize_1)
count_vect.fit(fixed_text)

counts = count_vect.transform(lemma_tokenizer)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())


predictions = cross_val_predict(text_clf, fixed_text, fixed_target)
log(run, fixed_text, fixed_target, predictions)
import pandas as pd
import numpy as np
import wandb

run = wandb.init(job_type='eval')
config = run.config
config.lowercase=True
config.ngram_min=1
config.ngram_max=1

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(lowercase=config.lowercase,
                             ngram_range=(config.ngram_min,
                                          config.ngram_max)
                                        )
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score, cross_val_predict

scores = cross_val_score(nb, counts, fixed_target)
print(scores)
print(scores.mean())

predictions = cross_val_predict(nb, counts, fixed_target)
wandb.log({"Accuracy": scores.mean()})
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import numpy as np
from nltk.tokenize import TweetTokenizer

from wandblog import log
import wandb
run = wandb.init()
config = run.config
tknzr = TweetTokenizer()

def tokenizer(doc):
    return tknzr.tokenize(doc)

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

count_vect = CountVectorizer(tokenizer=tokenizer)
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score, cross_val_predict

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())

predictions = cross_val_predict(nb, counts, fixed_target)
log(run, fixed_text, fixed_target, predictions)
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem.snowball import SnowballStemmer

from wandblog import log
import wandb
run = wandb.init()
config = run.config

stemmer = SnowballStemmer("english")
def tokenizer(doc):
    stemmed_words = [stemmer.stem(word) for word in word_tokenize(doc)]
    return stemmed_words

import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

count_vect = CountVectorizer(tokenizer=tokenizer)
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score, cross_val_predict

scores = cross_val_score(nb, counts, fixed_target)
print(scores)
print(scores.mean())

predictions = cross_val_predict(nb, counts, fixed_target)
log(run, fixed_text, fixed_target, predictions)
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import wordnet as wn

from wandblog import log
import wandb
run = wandb.init()
config = run.config

def tokenizer(doc):
    tokens = []
    for word in word_tokenize(doc):
        for synset in wn.synsets(word):
            tokens.append(synset)
    return tokens

import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

count_vect = CountVectorizer(tokenizer=tokenizer)
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score, cross_val_predict

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())

predictions = cross_val_predict(nb, counts, fixed_target)
log(run, fixed_text, fixed_target, predictions)
import pandas as pd
import numpy as np

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline

from wandblog import log
import wandb
run = wandb.init()
config = run.config

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer


clf = SGDClassifier()

from sklearn.model_selection import cross_val_score, cross_val_predict

text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),
                      ('tfidf', TfidfTransformer()),
                      ('clf', SGDClassifier(max_iter=1000)),
 ])

scores = cross_val_score(text_clf, fixed_text, fixed_target)
print(scores)
print(scores.mean())

predictions = cross_val_predict(text_clf, fixed_text, fixed_target)
log(run, fixed_text, fixed_target, predictions)
import pandas as pd
import numpy as np
import wandb
from wandblog import log

run = wandb.init()
config = run.config   # for tracking model inputs
summary = run.summary # for tracking model outputs

config.lowercase = False
config.alpha = 1.0

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(lowercase = config.lowercase)
count_vect.fit(fixed_text)
counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB(alpha = config.alpha)

from sklearn.model_selection import cross_val_score, cross_val_predict
scores = cross_val_score(nb, counts, fixed_target, cv=20)

print(scores)
print(scores.mean())

predictions = cross_val_predict(nb, counts, fixed_target)
log(run, fixed_text, fixed_target, predictions)

import pandas as pd
import numpy as np
import re, string

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

def tokenize_0(text):
    text = re.sub(r'[^\w\s]','',text) # remove punctuation
    return re.compile(r'\W+').split(text) # split at non words

def tokenize_1(text):
    return text.split()

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(tokenizer=tokenize_0)
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())
from sklearn import datasets

digits = datasets.load_digits()

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import cross_val_score

nb = MultinomialNB()

scores = cross_val_score(nb, digits.data, digits.target, cv=10)
print(scores)
print(scores.mean())


import numpy as np

from sklearn.base import BaseEstimator, TransformerMixin

#import gensim
# let X be a list of tokenized texts (i.e. list of lists of tokens)
#model = gensim.models.Word2Vec(X, size=100)
#w2v = dict(zip(model.wv.index2word, model.wv.syn0))

class MeanEmbeddingVectorizer(BaseEstimator, TransformerMixin):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # pull out a single value from the dictionary and check its dimension
        # this is a little ugly to make it work in python 2 and 3
        self.dim = len(next(iter(word2vec.values())))

    def fit(self, X, y):
        return self

    def transform(self, X):
        mean_vects = []
        for words in X:
            word_vects = []
            for w in words:
                if w in self.word2vec:
                    word_vects.append(self.word2vec[w])

            mean_vect = np.mean(word_vects, axis=0)
            mean_vects.append(np.array(mean_vect))

        mean_vects = np.array(mean_vects)

        return mean_vects
import pandas as pd
import csv
import numpy as np

words = pd.read_table('glove/glove.6B.50d.txt', sep=" ", index_col=0, header=None, quoting=csv.QUOTE_NONE)

def vec(w):
  return words.loc[w].as_matrix()

words_matrix = words.as_matrix()

def find_n_closest_words(v, n):
  diff = words_matrix - v
  delta = np.sum(diff * diff, axis=1)
  #i = np.argmin(delta)
  idx = (delta).argsort()[:n]
  close_words = []
  for i in idx:
    close_words.append(words.iloc[i].name)
  return close_words

print(find_n_closest_words( vec('srtgsrtg'), 5 ))
# First attempt at feature extraction
# Leads to an error, can you tell why?

import pandas as pd
import numpy as np

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

from sklearn.feature_extraction.text import CountVectorizer

count_vect=CountVectorizer()
count_vect.fit(text[:10])
# second attempt at feature extraction

import pandas as pd
import numpy as np

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

# what did we do here?
fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

# print the number of words in the vocabulary
print(len(count_vect.vocabulary_))
import pandas as pd
import numpy as np

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer(lowercase=True)
count_vect.fit(fixed_text)

# turns the text into a sparse matrix
counts = count_vect.transform(fixed_text)

my_counts = count_vect.transform(["love that iphone!", "HATE that iphone"])
print(my_counts)
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),
                ('feature_selection', SelectKBest(chi2, k=10000)),
                ('multinomialnb', MultinomialNB())])

p.fit(fixed_text, fixed_target)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(p, fixed_text, fixed_target, cv=10)
print(scores)
print(scores.mean())

import pandas as pd
import numpy as np
import wandb

run = wandb.init()
config = run.config
summary = run.summary

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.svm import LinearSVC

p = Pipeline(steps=[('counts', CountVectorizer()),
                ('svm', LinearSVC())])

from sklearn.model_selection import GridSearchCV

# these parameters define the different configurations we are going to try
# in our grid search

parameters = {
#    'counts__max_df': (0.5, 0.75,1.0),
#    'counts__min_df': (0,1,2),
#    'counts__token_pattern': ('(?u)\b\w\w+\b', '(?u)\b\w\w+\b'),
#    'counts__lowercase' : (True, False),
#    'counts__ngram_range': ((1,1), (1,2)),
#    'feature_selection__k': (1000, 10000, 100000)
#    'multinomialnb__alpha': (0.5, 1)
    'svm__max_iter': (1000, 10000)
    }

# setup the grid search - increasing n_jobs will make more threads
grid_search = GridSearchCV(p, parameters, n_jobs=1, verbose=2, cv=2)

# do the actual grid search
grid_search.fit(fixed_text, fixed_target)

# output the results and the parameters that obtained those results
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))
    config[param_name] = best_parameters[param_name]

summary['accuracy'] = grid_search.best_score_ * 100
import pandas as pd
import numpy as np
import wandb

run = wandb.init()
config = run.config
summary = run.summary

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

p = Pipeline(steps=[('counts', CountVectorizer()),
                ('multinomialnb', MultinomialNB())])

from sklearn.model_selection import GridSearchCV

# these parameters define the different configurations we are going to try
# in our grid search

parameters = {
#    'counts__max_df': (0.5, 0.75,1.0),
#    'counts__min_df': (0,1,2),
#    'counts__token_pattern': ('(?u)\b\w\w+\b', '(?u)\b\w\w+\b'),
    'counts__lowercase' : (True, False),
    'counts__ngram_range': ((1,1), (1,2)),
#    'feature_selection__k': (1000, 10000, 100000)
    'multinomialnb__alpha': (0.5, 1)
    }

# setup the grid search - increasing n_jobs will make more threads
grid_search = GridSearchCV(p, parameters, n_jobs=1, verbose=2, cv=2)

# do the actual grid search
grid_search.fit(fixed_text, fixed_target)

# output the results and the parameters that obtained those results
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))
    config[param_name] = best_parameters[param_name]

summary['accuracy'] = grid_search.best_score_
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

p = Pipeline(steps=[('counts', CountVectorizer()),
                ('multinomialnb', MultinomialNB())])

from sklearn.model_selection import GridSearchCV



parameters = {
#    'counts__max_df': (0.5, 0.75,1.0),
#    'counts__min_df': (0,1,2),
#    'counts__token_pattern': ('(?u)\b\w\w+\b', '(?u)\b\w\w+\b'),
    'counts__lowercase' : (True, False),
    'counts__ngram_range': ((1,1), (1,2)),
#    'feature_selection__k': (1000, 10000, 100000)
    'multinomialnb__alpha': (0.5, 1, 2)
    }

grid_search = GridSearchCV(p, parameters, n_jobs=1, verbose=2, cv=10)

grid_search.fit(fixed_text, fixed_target)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))
# Quick example of loading our data into variables

import pandas as pd
import numpy as np

# Puts tweets into a data frame
df = pd.read_csv('tweets.csv')

# Selects the first column from our data frame
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# Selects the third column from our data frame
text = df['tweet_text']

print(len(text))

import json
 
from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils

import numpy as np

import pandas as pd
import numpy as np

import wandb
from wandb.wandb_keras import WandbKerasCallback

run = wandb.init()
config = run.config

config.max_words = 1000
config.max_length = 300
# Puts tweets into a data frame
df = pd.read_csv('tweets.csv')

target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text'].astype(str)

category_to_num = {"I can't tell": 0, "Negative emotion": 1, "Positive emotion": 2, "No emotion toward brand or product": 3}
target_num = [category_to_num[t] for t in target]
target_one_hot = np_utils.to_categorical(target_num)

tokenizer = Tokenizer(num_words=config.max_words)
tokenizer.fit_on_texts(list(text))
sequences = tokenizer.texts_to_sequences(list(text))
data = pad_sequences(sequences, maxlen=config.max_length)

train_data = data[:6000]
test_data = data[6000:]
train_target = target_one_hot[:6000]
test_target = target_one_hot[6000:]

model = Sequential()
model.add(Embedding(config.max_words, 128, input_length=300))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(4, activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(train_data, train_target, callbacks=[WandbKerasCallback()], validation_data=(test_data, test_target))
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),
                ('multinomialnb', MultinomialNB())])

p.fit(fixed_text, fixed_target)
print(p.predict(["I love my iphone!"]))
# Take a look at the data using pandas

import pandas as pd
import numpy as np

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']
product = df['emotion_in_tweet_is_directed_at']

# For more ideas check out
#    https://pandas.pydata.org/pandas-docs/stable/10min.html
#print(text.head())

# Prints the string where the target contains the word Positive
#print(text[target.str.contains("Negative")])

# Other ideas
#print(target[0])
#print(text.str.contains("iphone"))
print(target.value_counts())
#print(text[0])
from sklearn import datasets

digits = datasets.load_digits()

from sklearn.model_selection import cross_val_score

from sklearn.linear_model import Perceptron

perceptron = Perceptron()

scores = cross_val_score(perceptron, digits.data, digits.target, cv=10)
print(scores)
print(scores.mean())

import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.linear_model import Perceptron

perceptron = Perceptron()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(perceptron, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 1))),
                ('multinomialnb', MultinomialNB())])

p.fit(fixed_text, fixed_target)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(p, fixed_text, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),
                ('multinomialnb', MultinomialNB(alpha=0))])

p.fit(fixed_text, fixed_target)
print(p.predict(["I love my iphone!"]))
#print(p.named_steps['counts'].vocabulary_.get(u'garage sale'))
#print(len(p.named_steps['counts'].vocabulary_))
import pandas as pd
import numpy as np

from sklearn.base import BaseEstimator, TransformerMixin

class NumBangExtractor(BaseEstimator, TransformerMixin):

    def __init__(self):
        pass

    def num_bang(self, str):
        """Helper code to compute number of exclamation points"""
        return str.count('!')

    def transform(self, X, y=None):
        counts = []
        for x in X:
            counts.append([self.num_bang(x)])    # the [] are important!
        return counts

    def fit(self, df, y=None):
        """Returns `self` unless something different happens in train and test"""
        return self

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline, FeatureUnion

p = Pipeline(steps=[('feats', FeatureUnion([
                            ('counts_preserve_case', CountVectorizer(lowercase=False)),
                            ('counts_lower_case', CountVectorizer(lowercase=True)),
                            ('numbang', NumBangExtractor())
                            ])),
                ('multinomialnb', MultinomialNB())])

from sklearn.model_selection import cross_val_score

scores = cross_val_score(p, fixed_text, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np
from sklearn.externals import joblib

df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),
                ('multinomialnb', MultinomialNB())])

p.fit(fixed_text, fixed_target)

joblib.dump(p, 'sentiment-model.pkl')
# need to run pip install flask

from sklearn.externals import joblib
from flask import Flask, request
from jinja2 import Template

p = joblib.load('sentiment-model.pkl')

app = Flask(__name__)

def pred(text):
    return p.predict([text])[0]

@app.route('/')
def index():
    text = request.args.get('text')
    if text:
        prediction = pred(text)
    else:
        prediction = ""

    template = Template("""
    <html>
        <body>
            <h1>Scikit Model Server</h1>
            <form>
                <input type="text" name="text">
                <input type="submit" >
            </form>
            <p>Prediction: {{ prediction }}</p>
        </body>
    </html>
    """)
    return template.render(prediction=prediction)





if __name__ == '__main__':
    app.run(port=8000)
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

p = Pipeline(steps=[('counts', CountVectorizer()),
                ('multinomialnb', MultinomialNB())])

p.fit(fixed_text, fixed_target)
print(p.predict(["I love my iphone!"]))
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

p = Pipeline(steps=[('counts', CountVectorizer()),
                ('feature_selection', SelectKBest(chi2, k='all')),
                ('multinomialnb', MultinomialNB())])

from sklearn.model_selection import GridSearchCV

# these parameters define the different configurations we are going to try
# in our grid search

parameters = {
#    'counts__max_df': (0.5, 0.75,1.0),
#    'counts__min_df': (0,1,2),
#    'counts__token_pattern': ('(?u)\b\w\w+\b', '(?u)\b\w\w+\b'),
    'counts__lowercase' : (True, False),
    'counts__ngram_range': ((1,1), (1,2)),
#    'feature_selection__k': (1000, 10000, 100000)
    'multinomialnb__alpha': (0.5, 1)
    }

# setup the grid search - increasing n_jobs will make more threads
grid_search = GridSearchCV(p, parameters, n_jobs=2, verbose=2, cv=2)

# do the actual grid search
grid_search.fit(fixed_text, fixed_target)

# output the results and the parameters that obtained those results
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))
import pandas as pd
import numpy as np


# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with a Naive Bayes classifier:
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(counts, target)

# See what the classifier predicts for some new tweets:
predictions = nb.predict(counts)
correct_predictions = sum(predictions == target)
print('Percent correct: ', 100.0 * correct_predictions / len(predictions))
import pandas as pd
import numpy as np


# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# The rows of  the "emotion" column have one of three strings:
# 'Positive emotion'
# 'Negative emotion'
# 'No emotion toward brand or product'

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)
print(counts.shape)

# Train with this data with a Naive Bayes classifier:
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

# (Tweets 0 to 5999 are used for training data)
nb.fit(counts[0:6000], target[0:6000])

# See what the classifier predicts for some new tweets:
# (Tweets 6000 to 9091 are used for testing)
predictions = nb.predict(counts[6000:9092])
print(len(predictions))
correct_predictions = sum(predictions == target[6000:9092])
print('Percent correct: ', 100.0 * correct_predictions / 3092)
import pandas as pd
import numpy as np


# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with a Naive Bayes classifier:
from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()

# (Tweets 0 to 5999 are used for training data)
nb.fit(counts[0:6000], target[0:6000])

# See what the classifier predicts for some new tweets:
# (Tweets 6000 to 9091 are used for testing)
predictions = nb.predict(counts[6000:9092])
correct_predictions = sum(predictions == target[6000:9092])
incorrect_predictions = (9092 - 6000) - correct_predictions
print('# of correct predictions: ' + str(correct_predictions))
print('# of incorrect predictions: ' + str(incorrect_predictions))
print('Percent correct: ' + str(100.0 * correct_predictions / (correct_predictions + incorrect_predictions)))

from sklearn.metrics import confusion_matrix
## We're ignoring "I can't tell" here for simplicity
label_list = ['Positive emotion', 'No emotion toward brand or product', 'Negative emotion']
cm = confusion_matrix(target[6000:9092], predictions, labels=label_list)
print("Labels in data:")
print(label_list)
print("Rows: actual labels, Columns: Predicted labels")
print(cm)
import pandas as pd
import numpy as np
from sklearn.naive_bayes import MultinomialNB

# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# The rows of  the "emotion" column have one of three strings:
# 'Positive emotion'
# 'Negative emotion'
# 'No emotion toward brand or product'

# Remove the blank rows from the series:
fixed_target = target[pd.notnull(text)]
fixed_text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)
counts = count_vect.transform(fixed_text)

# Train with this data with a dummy classifier:
from sklearn.dummy import DummyClassifier
nb = DummyClassifier(strategy='most_frequent')

from sklearn.model_selection import cross_val_score

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(lowercase=False)
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB(alpha=0.5)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(nb, counts, fixed_target, cv=20)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(clf, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.linear_model import SGDClassifier
clf = SGDClassifier()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(clf, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())
import pandas as pd
import numpy as np


df = pd.read_csv('tweets.csv')
target = df['is_there_an_emotion_directed_at_a_brand_or_product']
text = df['tweet_text']

fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.dummy import DummyClassifier

nb = DummyClassifier(strategy="most_frequent")

from sklearn.model_selection import cross_val_score

scores = cross_val_score(nb, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())


import pandas as pd
import numpy as np


# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# The rows of  the "emotion" column have one of three strings:
# 'Positive emotion'
# 'Negative emotion'
# 'No emotion toward brand or product'

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with a Dummy classifier:
from sklearn.dummy import DummyClassifier
nb = DummyClassifier(strategy='most_frequent')

# (Tweets 0 to 5999 are used for training data)
nb.fit(counts[0:6000], target[0:6000])



# See what the classifier predicts for some new tweets:
# (Tweets 6000 to 9091 are used for testing)
predictions = nb.predict(counts[6000:9092])
correct_predictions = sum(predictions == target[6000:9092])
print('Percent correct: ', 100.0 * correct_predictions / 3092.0)
import pandas as pd
import numpy as np
from wandblog import log

# Get a pandas DataFrame object of all the data in the csv file:
df = pd.read_csv('tweets.csv')

# Get pandas Series object of the "tweet text" column:
text = df['tweet_text']

# Get pandas Series object of the "emotion" column:
target = df['is_there_an_emotion_directed_at_a_brand_or_product']

# Remove the blank rows from the series:
target = target[pd.notnull(text)]
text = text[pd.notnull(text)]

# Perform feature extraction:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(text)
counts = count_vect.transform(text)

# Train with this data with a Naive Bayes classifier:
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(counts, target)

# See what the classifier predicts for some new tweets:
predictions = nb.predict(counts)
print(len(predictions))
correct_predictions = sum(predictions == target)
print('Percent correct: ', 100.0 * correct_predictions / len(predictions))

log(text, target, predictions)
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

wnl = WordNetLemmatizer()

def tokenizer(doc):
    return [t for t in word_tokenize(doc)]

class Tokenizer(object):

    def fit(self, X, y):
        return self

    def transform(self, X):
        return_data = [tokenizer(x) for x in X]
        return return_data



def log(run, text, target, predictions):
    correct_predictions = sum(predictions == target)

#    run.examples.set_columns((
#        ('tweet', str),
#        ('target', str),
#        ('prediction', str),
#        ('accuracy', float)))

    run.summary['accuracy']=(100.0 * correct_predictions / len(predictions))


#    for idx, (tweet, t, prediction)  in enumerate(zip(text[:100], target[:100], predictions[:100])):
#        run.examples.add({
#                'tweet': tweet,
#                'target': t,
#                'prediction': prediction,
#                'accuracy': (prediction == tweet),
#        })
import nltk
from nltk.corpus import wordnet as wn
nltk.download('wordnet')

print(wn.synsets('dog'))
dog = wn.synset('domestic_animal.n.01')

print(dog.hypernyms())
#print(dog.hyponyms())
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
import subprocess
import os

import wandb
from wandb.keras import WandbCallback

run = wandb.init()
config = run.config
config.img_size = 100
config.batch_size = 64
config.epochs = 25
config.team = None

if config.team is None:
    raise ValueError("You must set config.team on line 16!")

# download the data if it doesn't exist
if not os.path.exists("simpsons"):
    print("Downloading Simpsons dataset...")
    subprocess.check_output(
        "curl https://storage.googleapis.com/wandb-production.appspot.com/mlclass/simpsons.tar.gz | tar xvz", shell=True)

# this is the augmentation configuration we will use for training
# see: https://keras.io/preprocessing/image/#imagedatagenerator-class
train_datagen = ImageDataGenerator(
    rescale=1./255)

# only rescaling augmentation for testing:
test_datagen = ImageDataGenerator(rescale=1./255)

# this is a generator that will read pictures found in
# subfolers of 'data/train', and indefinitely generate
# batches of augmented image data
train_generator = train_datagen.flow_from_directory(
    'simpsons/train',  # this is the target directory
    target_size=(config.img_size, config.img_size),
    batch_size=config.batch_size)

# this is a similar generator, for validation data
test_generator = test_datagen.flow_from_directory(
    'simpsons/test',
    target_size=(config.img_size, config.img_size),
    batch_size=config.batch_size)

labels = list(test_generator.class_indices.keys())

model = Sequential()
model.add(Conv2D(16, (3, 3), input_shape=(
    config.img_size, config.img_size, 3), activation="relu"))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(13, activation="softmax"))
model.compile(optimizer=optimizers.Adam(),
              loss='categorical_crossentropy', metrics=['accuracy'])

model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=config.epochs,
    workers=4,
    validation_data=test_generator,
    callbacks=[WandbCallback(
        data_type="image", labels=labels, generator=test_generator, save_model=False)],
    validation_steps=len(test_generator))
